---
title: "Part 1: Create AI Scraping Flow"
description: Build the core AI web scraper workflow with database setup, task functions, and pgflow orchestration
sidebar:
  order: 2
---

import { Steps } from '@astrojs/starlight/components';
import JoinCommunity from '@/components/JoinCommunity.astro';

Let's build the backend workflow that powers your AI web scraper.

:::note[New to pgflow?]
If you haven't installed pgflow yet, check the [installation guide](/getting-started/install-pgflow/) first.
:::

What we'll create:

<Steps>
1. Database table for results
2. Four task functions (scrape, summarize, tag, save)
3. pgflow workflow connecting them
4. Edge Worker to run it
5. Test everything locally
</Steps>

## Step 1 - Create `websites` table

Set up your database to store AI analysis results.

<Steps>
1. Create a new migration:

   ```bash
   npx supabase migration new add_websites
   ```

2. Add this SQL to the generated file:

   ```sql
   create table public.websites (
     id           bigserial primary key,
     website_url  text not null,
     summary      text,
     tags         text[],
     created_at   timestamptz default now()
   );
   ```

3. Apply it:

   ```bash
   npx supabase migrations up --local
   ```

   :::note
   Make sure `supabase start` is running before applying migrations. The SQL file will be created at `supabase/migrations/<timestamp>_add_websites.sql`.
   :::
</Steps>

---

## Step 2 - Create task functions

Build four focused functions that each do one thing well:

| File | What it does |
| --- | --- |
| `scrapeWebsite.ts` | Fetches webpage content |
| `summarize.ts` | AI summary |
| `extractTags.ts` | AI tags |
| `saveWebsite.ts` | Saves to database |

Put these in `supabase/functions/_tasks` (see [organizing flows code](/how-to/organize-flows-code/) for project structure):

### Web Scraping

<details open>
  <summary><strong>scrapeWebsite.ts</strong> - Fetch and clean webpage content</summary>

```typescript
// supabase/functions/_tasks/scrapeWebsite.ts
export default async function scrapeWebsite(url: string) {
  console.log("[scrapeWebsite] fetching", url);

  const res = await fetch(url, {
    signal: AbortSignal.timeout(10000)  // 10 second timeout
  });

  if (!res.ok) throw new Error(`Fetch failed: ${res.status}`);

  const html = await res.text();
  const text = html.replace(/<[^>]+>/g, " ").replace(/\s+/g, " ").trim();

  return { content: text.slice(0, 25_000) };  // limit tokens
}
```

:::caution[Production HTML Processing]
In production, use [html-to-text](https://www.npmjs.com/package/html-to-text) for full HTML parsing.
:::
</details>

### AI Analysis

Two OpenAI functions that return structured, type-safe data:

:::tip[Structured Outputs]
We're using OpenAI's newer Responses API (`openai.responses.parse`) rather than traditional Chat Completions. This provides [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs) for reliable, validated responses. Note that `openai.responses.parse` will throw an exception if the model output doesn't match your Zod schema.
:::

<details>
  <summary><strong>summarize.ts</strong> - AI summary</summary>

```typescript
// supabase/functions/_tasks/summarize.ts
import OpenAI from "jsr:@openai/openai@4.33.0";
import { z } from "jsr:@zod/zod";
import { zodTextFormat } from "jsr:@openai/openai@4.33.0/helpers/zod";

const apiKey = Deno.env.get("OPENAI_API_KEY");
if (!apiKey) throw new Error("Missing OPENAI_API_KEY");

const openai = new OpenAI({ apiKey });
const SummarySchema = z.object({ summary: z.string() });

export default async function summarize(content: string) {
  console.log("[summarize] processing content");

  const response = await openai.responses.parse({
    model: "gpt-4o",
    input: [
      { role: "system", content: "Return a short paragraph summary." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(SummarySchema, "summary") },
  });

  return response.output_parsed.summary;
}
```
</details>

<details>
  <summary><strong>extractTags.ts</strong> - Extract tags</summary>

```typescript
// supabase/functions/_tasks/extractTags.ts
import OpenAI from "jsr:@openai/openai@4.33.0";
import { z } from "jsr:@zod/zod";
import { zodTextFormat } from "jsr:@openai/openai@4.33.0/helpers/zod";

const apiKey = Deno.env.get("OPENAI_API_KEY");
if (!apiKey) throw new Error("Missing OPENAI_API_KEY");

const openai = new OpenAI({ apiKey });
const TagsSchema = z.object({ tags: z.array(z.string()).max(10) });

export default async function extractTags(content: string) {
  console.log("[extractTags] extracting tags");

  const response = await openai.responses.parse({
    model: "gpt-4o",
    input: [
      { role: "system", content: "Return 5-10 descriptive tags." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(TagsSchema, "tags") },
  });

  return response.output_parsed.tags;
}
```
</details>

### Save to Database

The final task saves all results to your database:

<details>
  <summary><strong>saveWebsite.ts</strong> - Store results</summary>

```typescript
import { createClient } from "@supabase/supabase-js";

const supabaseUrl = Deno.env.get("SUPABASE_URL");
const supabaseKey = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY");

if (!supabaseUrl || !supabaseKey) {
  throw new Error("Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY");
}

const supabase = createClient(supabaseUrl, supabaseKey);

export default async function saveWebsite(row: {
  website_url: string; summary: string; tags: string[];
}) {
  console.log("[saveWebsite] inserting row");

  const { data } = await supabase
    .from("websites")
    .insert(row)
    .select("*")
    .single()
    .throwOnError();

  console.log("[saveWebsite] inserted with id:", data?.id);
  return data;
}
```

:::note
Uses service role key for direct database access. Both URL and key are auto-available in Edge Functions.
:::
</details>

---

## Step 3 - Define flow

Connect tasks into a workflow using pgflow's [TypeScript DSL](/concepts/flow-dsl/) (`supabase/functions/_flows/analyze_website.ts`):

```typescript title="supabase/functions/_flows/analyze_website.ts"
import { Flow } from "@pgflow/dsl";
import scrapeWebsite   from "../_tasks/scrapeWebsite.ts";
import summarize from "../_tasks/summarize.ts";
import extractTags     from "../_tasks/extractTags.ts";
import saveWebsite     from "../_tasks/saveWebsite.ts";

type Input = { url: string };

export default new Flow<Input>({ slug: "analyze_website", maxAttempts: 3 })
  .step({ slug: "website" },
    ({ run })      => scrapeWebsite(run.url))

  .step({ slug: "summary", dependsOn: ["website"] },
    ({ website })  => summarize(website.content))  // ðŸ‘ˆ Type inferred automatically!

  .step({ slug: "tags",    dependsOn: ["website"] },
    ({ website })  => extractTags(website.content))

  .step({ slug: "saveToDb", dependsOn: ["summary", "tags"] },
    ({ run, summary, tags }) =>
      saveWebsite({ website_url: run.url, summary, tags }));
```

Flow structure:
- `website` â†’ fetches the URL (root step)
- `summary` & `tags` â†’ run in parallel (both need website content)  
- `saveToDb` â†’ waits for both, then saves everything

Summary and tags execute simultaneously since both only need website content - cutting execution time in half. All state transitions happen transactionally in the database, ensuring your flow never ends up in an inconsistent state even if tasks fail or workers crash.

:::note
`maxAttempts: 3` means each step retries up to 3 times on failure.
:::

---

## Step 4 - Compile & migrate

Turn your TypeScript flow into SQL using pgflow's [compiler](/getting-started/compile-to-sql/):

```bash
# TypeScript â†’ SQL
npx pgflow@latest compile supabase/functions/_flows/analyze_website.ts

# Apply to database
npx supabase migrations up --local
```

The DSL compiler extracts your flow's shape (steps, dependencies) and generates SQL that inserts it into the database. **The database's flow definition determines what runs and when.** The TypeScript DSL also wires up step handlers so the Edge Worker knows which function to invoke for each step. Learn more about [how pgflow works](/concepts/how-pgflow-works/).

:::danger[Adding or removing steps]
Changing steps requires a new flow with unique `slug`. This is a current limitation, but workarounds for local development are planned to make iteration easier. See [versioning](/how-to/version-your-flows/). For development, you can [delete a flow and its data](/how-to/delete-flow-and-data/) to start fresh.
:::

---

## Step 5 - Setup Edge Worker

Create a worker function that will process steps from your flow:

```bash
npx supabase functions new analyze_website_worker
```

Replace the generated `index.ts` with the following code:

```typescript title="supabase/functions/analyze_website_worker/index.ts"
import { EdgeWorker } from "jsr:@pgflow/edge-worker";
import AnalyzeWebsite from '../_flows/analyze_website.ts';

EdgeWorker.start(AnalyzeWebsite);  // That's it! ðŸ¤¯
```

and update your `supabase/config.toml`:

```diff title="supabase/config.toml"
  [functions.analyze_website_worker]
  enabled = true
- verify_jwt = true
+ verify_jwt = false
```

:::caution
Only disable JWT for local development.
:::

:::note[Worker magic]
EdgeWorker handles polling, retries, and failures automatically. Your code just defines the flow shape!

See more in [Run your flow](/getting-started/run-flow/) documentation page
:::

---

## Step 6 - Run & test

Start everything:

<Steps>
1. Start Supabase:
   ```bash frame="none"
   npx supabase start
   ```

2. Serve Edge Functions:
   ```bash frame="none"
   npx supabase functions serve
   ```

3. Start the worker (keep this terminal open):
   ```bash frame="none"
   curl -X POST http://127.0.0.1:54321/functions/v1/analyze_website_worker
   ```
</Steps>

Trigger the flow in SQL Editor:

```sql
select * from pgflow.start_flow(
  flow_slug => 'analyze_website',
  input     => '{"url":"https://supabase.com"}'
);
```

### What happens

Worker output:
```
[scrapeWebsite] fetching https://supabase.com
[summarize] processing content
[extractTags] extracting tags
[saveWebsite] inserting row
[saveWebsite] inserted with id: 1
```

Check your database:
```sql
select website_url, tags, summary from websites;
```

:::tip[Monitor with simple SQL queries]
Check flow status anytime:

```sql
SELECT * FROM pgflow.runs WHERE flow_slug = 'analyze_website'
```

See more in [Monitor flow execution](/how-to/monitor-flow-execution/) documentation page
:::

---

## What you've built

Your AI scraper pipeline:

- **Auto-retries** - Failed steps retry up to 3 times
- **Parallel AI** - Summary and tags run simultaneously  
- **Full history** - Every run tracked in your database
- **Modular code** - Each task is independent and testable
- **ACID guarantees** - Built on pgmq, a real Postgres queue with transactional safety


---

## Troubleshooting

<details>
  <summary>Common issues</summary>

| Error | Fix |
|-------|-----|
| `SASL_SIGNATURE_MISMATCH` | URL-encode DB password |
| `Missing OPENAI_API_KEY` | Add to `supabase/functions/.env` |
| `401 (Unauthorized)` | Check OpenAI key is valid |
| Compile errors | Run `npx pgflow@latest compile --help` |

</details>

For debugging, see [Monitor flow execution](/how-to/monitor-flow-execution/).

---

<JoinCommunity />
