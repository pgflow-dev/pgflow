---
title: "Part 1: Create AI Scraping Flow"
description: Build the core AI web scraper workflow with database setup, task functions, and pgflow orchestration
sidebar:
  order: 2
---

import { Steps } from '@astrojs/starlight/components';
import JoinCommunity from '@/components/JoinCommunity.astro';

Let's build the backend workflow that powers your AI web scraper.

:::note[Before You Start]
Make sure you've checked the [tutorial introduction](/tutorials/ai-web-scraper/) first for all prerequisites, required tools, and version information.
:::

What we'll create:

<Steps>
1. Database table for results
2. Four task functions (scrape, summarize, tag, save)
3. pgflow workflow connecting them
4. Compile flow to SQL and migrate
5. Setup Edge Worker to run it
6. Test everything locally
</Steps>

## Step 1 - Create `websites` table

Set up your database to store AI analysis results.

<Steps>
1. Create a new migration:

   ```bash
   npx supabase migration new add_websites
   ```

2. Add this SQL to the generated file:

   ```sql
   create table public.websites (
     id           bigserial primary key,
     website_url  text not null,
     summary      text,
     tags         text[],
     created_at   timestamptz default now()
   );
   ```

3. Apply it:

   ```bash
   npx supabase migrations up --local
   ```

   :::note
   Make sure `supabase start` is running before applying migrations. The SQL file will be created at `supabase/migrations/<timestamp>_add_websites.sql`.
   :::
</Steps>

---

## Step 2 - Create task functions

Build four focused functions that each do one thing well:

| File | What it does |
| --- | --- |
| `scrapeWebsite.ts` | Fetches webpage content |
| `summarize.ts` | AI summary |
| `extractTags.ts` | AI tags |
| `saveWebsite.ts` | Saves to database |

Put these in `supabase/functions/_tasks` (see [organizing flows code](/how-to/organize-flows-code/) for project structure):

### Web Scraping

<details open>
  <summary><strong>scrapeWebsite.ts</strong> - Fetch and clean webpage content</summary>

```typescript
// supabase/functions/_tasks/scrapeWebsite.ts
export default async function scrapeWebsite(url: string) {
  console.log("[scrapeWebsite] fetching", url);

  const res = await fetch(url, {
    signal: AbortSignal.timeout(10000)  // 10 second timeout
  });

  if (!res.ok) throw new Error(`Fetch failed: ${res.status}`);

  const html = await res.text();
  const text = html.replace(/<[^>]+>/g, " ").replace(/\s+/g, " ").trim();

  return { content: text.slice(0, 25_000) };  // limit tokens
}
```

:::caution[Production HTML Processing]
In production, use [html-to-text](https://www.npmjs.com/package/html-to-text) for full HTML parsing.
:::
</details>

### AI Analysis

Two OpenAI functions that return structured, type-safe data:

:::tip[Structured Outputs]
We're using OpenAI's newer Responses API (`openai.responses.parse`) rather than traditional Chat Completions. This provides [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs) for reliable, validated responses. Note that `openai.responses.parse` will throw an exception if the model output doesn't match your Zod schema.
:::

<details>
  <summary><strong>summarize.ts</strong> - AI summary</summary>

```typescript
// supabase/functions/_tasks/summarize.ts
import OpenAI from "jsr:@openai/openai@4.33.0";
import { z } from "jsr:@zod/zod";
import { zodTextFormat } from "jsr:@openai/openai@4.33.0/helpers/zod";

const apiKey = Deno.env.get("OPENAI_API_KEY");
if (!apiKey) throw new Error("Missing OPENAI_API_KEY");

const openai = new OpenAI({ apiKey });
const SummarySchema = z.object({ summary: z.string() });

export default async function summarize(content: string) {
  console.log("[summarize] processing content");

  const response = await openai.responses.parse({
    model: "gpt-4o",
    input: [
      { role: "system", content: "Return a short paragraph summary." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(SummarySchema, "summary") },
  });

  return response.output_parsed.summary;
}
```
</details>

<details>
  <summary><strong>extractTags.ts</strong> - Extract tags</summary>

```typescript
// supabase/functions/_tasks/extractTags.ts
import OpenAI from "jsr:@openai/openai@4.33.0";
import { z } from "jsr:@zod/zod";
import { zodTextFormat } from "jsr:@openai/openai@4.33.0/helpers/zod";

const apiKey = Deno.env.get("OPENAI_API_KEY");
if (!apiKey) throw new Error("Missing OPENAI_API_KEY");

const openai = new OpenAI({ apiKey });
const TagsSchema = z.object({ tags: z.array(z.string()).max(10) });

export default async function extractTags(content: string) {
  console.log("[extractTags] extracting tags");

  const response = await openai.responses.parse({
    model: "gpt-4o",
    input: [
      { role: "system", content: "Return 5-10 descriptive tags." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(TagsSchema, "tags") },
  });

  return response.output_parsed.tags;
}
```
</details>

### Save to Database

The final task saves all results to your database:

<details>
  <summary><strong>saveWebsite.ts</strong> - Store results</summary>

```typescript
import { createClient } from "@supabase/supabase-js";

const supabaseUrl = Deno.env.get("SUPABASE_URL");
const supabaseKey = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY");

if (!supabaseUrl || !supabaseKey) {
  throw new Error("Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY");
}

const supabase = createClient(supabaseUrl, supabaseKey);

export default async function saveWebsite(row: {
  website_url: string; summary: string; tags: string[];
}) {
  console.log("[saveWebsite] inserting row");

  const { data } = await supabase
    .from("websites")
    .insert(row)
    .select("*")
    .single()
    .throwOnError();

  console.log("[saveWebsite] inserted with id:", data?.id);
  return data;
}
```

:::note
Uses service role key for direct database access. Both URL and key are auto-available in Edge Functions.
:::
</details>

---

## Step 3 - Define flow

Connect tasks into a workflow using pgflow's [TypeScript DSL](/concepts/flow-dsl/) (`supabase/functions/_flows/analyze_website.ts`):

```typescript title="supabase/functions/_flows/analyze_website.ts"
import { Flow } from "@pgflow/dsl";
import scrapeWebsite from "../_tasks/scrapeWebsite.ts";
import summarize from "../_tasks/summarize.ts";
import extractTags from "../_tasks/extractTags.ts";
import saveWebsite from "../_tasks/saveWebsite.ts";

type Input = { url: string };

export default new Flow<Input>({ slug: "analyze_website", maxAttempts: 3 })
  .step({ slug: "website" },
    ({ run }) => scrapeWebsite(run.url))

  .step({ slug: "summary", dependsOn: ["website"] },
    ({ website }) => summarize(website.content))  // ðŸ‘ˆ Type inferred automatically!

  .step({ slug: "tags", dependsOn: ["website"] },
    ({ website }) => extractTags(website.content))

  .step({ slug: "saveToDb", dependsOn: ["summary", "tags"] },
    ({ run, summary, tags }) => saveWebsite({ website_url: run.url, summary, tags }));
```

Flow structure:

<img src="/analyze_website.svg" alt="AI Web Scraper Workflow" title="AI-powered web scraper workflow DAG" className="responsive-svg" width="70%" />

- `website` â†’ fetches the URL (root step)
- `summary` & `tags` â†’ run in parallel (both need website content)
- `saveToDb` â†’ waits for both, then saves everything

Summary and tags execute simultaneously since both only need website content - cutting execution time in half. All state transitions happen transactionally in the database, ensuring your flow never ends up in an inconsistent state even if tasks fail or workers crash.

:::note
`maxAttempts: 3` means each step retries up to 3 times on failure.
:::

---

## Step 4 - Compile & migrate

Turn your TypeScript flow into SQL using pgflow's [compiler](/getting-started/compile-to-sql/):

<Steps>
1. Compile TypeScript to SQL:
   ```bash
   npx pgflow@latest compile supabase/functions/_flows/analyze_website.ts
   # Generates supabase/migrations/<timestamp>_analyze_website.sql
   ```

2. Apply migration to database:
   ```bash
   npx supabase migrations up --local
   ```
</Steps>

The DSL compiler extracts your flow's shape (steps, dependencies) and generates SQL that inserts it into the database. **The database's flow definition determines what runs and when.** The TypeScript DSL also wires up step handlers so the Edge Worker knows which function to invoke for each step. Learn more about [how pgflow works](/concepts/how-pgflow-works/).

:::danger[Adding or removing steps]
Changing steps requires a new flow with unique `slug`. This is a core design decision in pgflow to ensure idempotency and data integrity. See [versioning](/how-to/version-your-flows/) for best practices. While local development experience will be improved in future releases, for now you can [delete a flow and its data](/how-to/delete-flow-and-data/) to start fresh during development.
:::

---

## Step 5 - Setup Edge Worker

<Steps>
1. Create a worker function that will process steps from your flow:
   ```bash
   npx supabase functions new analyze_website_worker
   ```

2. Replace the generated `index.ts` with the following code:
   ```typescript title="supabase/functions/analyze_website_worker/index.ts"
   import { EdgeWorker } from "jsr:@pgflow/edge-worker";
   import AnalyzeWebsite from '../_flows/analyze_website.ts';

   EdgeWorker.start(AnalyzeWebsite);  // That's it! ðŸ¤¯
   ```

3. Update your `supabase/config.toml`:
   ```diff title="supabase/config.toml"
     [functions.analyze_website_worker]
     enabled = true
   - verify_jwt = true
   + verify_jwt = false
   ```
</Steps>

:::note[Worker magic]
EdgeWorker handles polling, retries, and failures automatically. Your code just defines the flow shape!

See more in [Run your flow](/getting-started/run-flow/) documentation page
:::

---

## Step 6 - Run & test

Start the services (make sure `supabase start` is already running):

<Steps>
1. Serve Edge Functions (keep this terminal open):
   ```bash frame="none"
   npx supabase functions serve
   ```

2. In a new terminal, start the worker:
   ```bash frame="none"
   curl -X POST http://127.0.0.1:54321/functions/v1/analyze_website_worker
   ```

   :::note[How the worker runs]
   The Edge Function ecosystem works like this:

   - `supabase functions serve` - Runs a server that hosts your Edge Functions
   - First curl request - Boots the worker, which stays alive and continuously polls the database for tasks
   - Subsequent curl requests - Have no effect (worker is already running)

   In production environments, workers are typically triggered by cron schedules rather than manual curl commands.
   :::

3. Trigger the flow in SQL Editor:
   ```sql
   select * from pgflow.start_flow(
     flow_slug => 'analyze_website',
     input     => '{"url":"https://supabase.com"}'
   );
   ```
</Steps>

### What happens

Worker output:
```
[scrapeWebsite] fetching https://supabase.com
[summarize] processing content
[extractTags] extracting tags
[saveWebsite] inserting row
[saveWebsite] inserted with id: 1
```

Check your database:
```sql
select website_url, tags, summary from websites;

-- Example output:
-- website_url          | tags                                               | summary
-- ---------------------|----------------------------------------------------|---------------------------------------------------------
-- https://supabase.com | {"postgres","api","backend","database","realtime"} | Supabase is an open source Firebase alternative providing
--                      |                                                    | a PostgreSQL database, authentication, instant APIs,
--                      |                                                    | realtime subscriptions, and storage.
```

:::tip[Monitor with simple SQL queries]
Check flow status anytime:

```sql
SELECT * FROM pgflow.runs WHERE flow_slug = 'analyze_website'
```

See more in [Monitor flow execution](/how-to/monitor-flow-execution/) documentation page
:::

---

## What you've built

Your AI scraper pipeline:

- **Auto-retries** - Failed steps retry up to 3 times
- **Parallel AI** - Summary and tags run simultaneously
- **Full history** - Every run tracked in your database
- **Modular code** - Each task is independent and testable
- **ACID guarantees** - Built on pgmq, a real Postgres queue with transactional safety


---

## Troubleshooting

<details>
  <summary>Common issues</summary>

| Error | Fix |
|-------|-----|
| `SASL_SIGNATURE_MISMATCH` | URL-encode DB password |
| `Missing OPENAI_API_KEY` | Add to `supabase/functions/.env` |
| `401 (Unauthorized)` | Check OpenAI key is valid |
| Compile errors | Run `npx pgflow@latest compile --help` |

</details>

For debugging, see [Monitor flow execution](/how-to/monitor-flow-execution/).

---

<JoinCommunity />
