---
title: AI Web Scraper
description: Build a workflow that scrapes websites, analyzes content with OpenAI, and saves to Postgres
sidebar:
  order: 1
---

import { Steps, LinkCard, CardGrid } from '@astrojs/starlight/components';
import JoinCommunity from '@/components/JoinCommunity.astro';

Build an AI-powered scraper: fetch → analyze → store

## What you'll build

You'll create a web scraper that:

<Steps>
1. Fetches webpage content
2. Analyzes it with OpenAI
3. Runs parallel AI operations  
4. Saves results to your database
</Steps>

<img src="/analyze_website.svg" alt="AI Web Scraper Workflow" title="AI-powered web scraper workflow DAG" className="responsive-svg" />

## Architecture overview

This tutorial covers building the backend workflow. Future tutorials will add a frontend UI and production deployment.

:::tip[Why pgflow for AI scraping?]
pgflow handles AI workflow complexity: automatic retries for API failures, parallel processing, and full observability.
Everything runs in your Supabase project - no external infrastructure needed.
:::

## Prerequisites

:::caution[Before you begin]

1. [Install pgflow](/getting-started/install-pgflow/) - it automatically sets up Edge Worker environment variables

2. Add your [OpenAI API key](https://platform.openai.com/api-keys) to `supabase/functions/.env`:
   ```diff
     # supabase/functions/.env
     EDGE_WORKER_DB_URL=postgres://...
     EDGE_WORKER_LOG_LEVEL=info
   + OPENAI_API_KEY=sk-...
   ```
:::

## What you'll learn

<Steps>
1. Create database tables for storing scraping results
2. Build modular task functions for web scraping
3. Integrate OpenAI with structured outputs for AI analysis
4. Design parallel DAG workflows for concurrent processing
5. Execute flows using Supabase Edge Functions
</Steps>

## Get started

<CardGrid>
  <LinkCard
    title="Part 1: Build the Backend"
    description="Create the database schema, AI tasks, and workflow"
    href="/tutorials/ai-web-scraper/backend/"
  />
</CardGrid>

<JoinCommunity />
