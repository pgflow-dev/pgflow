---
title: AI Web Scraper
description: Build a workflow that scrapes websites, analyzes content with OpenAI, and saves to Postgres
sidebar:
  order: 1
---

import { Steps, LinkCard, CardGrid } from '@astrojs/starlight/components';

Build an AI-powered scraper: fetch → analyze → store

## What you'll build

You'll create a web scraper that:

<Steps>
1. Fetches webpage content
2. Analyzes it with OpenAI
3. Runs parallel AI operations  
4. Saves results to your database
</Steps>

<img src="/analyze_website.svg" alt="AI Web Scraper Workflow" title="AI-powered web scraper workflow DAG" className="responsive-svg" />

## Architecture overview

This tutorial covers building the backend workflow. Future tutorials will add a frontend UI and production deployment.

:::tip[Why pgflow for AI scraping?]
pgflow handles AI workflow complexity: automatic retries for API failures, parallel processing, and full observability.
Everything runs in your Supabase project - no external infrastructure needed.
:::

## Prerequisites

:::caution[Before you begin]

1. [Install pgflow](/getting-started/install-pgflow/) - it automatically sets up Edge Worker environment variables

2. Add your [OpenAI API key](https://platform.openai.com/api-keys) to `supabase/functions/.env`:
   ```diff
     # supabase/functions/.env
     EDGE_WORKER_DB_URL=postgres://...
     EDGE_WORKER_LOG_LEVEL=info
   + OPENAI_API_KEY=sk-...
   ```
:::

## Get started

<CardGrid>
  <LinkCard
    title="Part 1: Build the Backend"
    description="Create the database schema, AI tasks, and workflow"
    href="/tutorials/ai-web-scraper/backend/"
  />
</CardGrid>

## What you'll learn

<Steps>
1. Creating flows, steps, and dependencies
2. Integrating OpenAI's API
3. Running parallel AI operations  
4. Handling errors with automatic retries
5. Testing and debugging workflows
</Steps>

## Discord

Questions? [Join Discord](https://discord.com/invite/NpffdEyb) to discuss pgflow.
