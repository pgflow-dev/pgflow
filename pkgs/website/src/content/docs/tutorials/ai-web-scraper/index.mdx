---
title: AI Web Scraper
description: Build a workflow that scrapes websites, analyzes content with OpenAI, and saves to Postgres
sidebar:
  order: 1
---

import { Steps, LinkCard, CardGrid } from '@astrojs/starlight/components';
import JoinCommunity from '@/components/JoinCommunity.astro';

:::note[Tech Stack]
- **Supabase Edge Functions** - Serverless runtime for workflow execution
- **Deno** - JavaScript/TypeScript runtime environment
- **pgflow** - Postgres-native workflow engine
- **OpenAI API** - AI content analysis
:::

Build an AI-powered scraper: fetch → analyze → store

## What you'll build

You'll create a practical AI web scraper workflow that:

<Steps>
1. Grabs content from any webpage with built-in error handling
2. Uses GPT-4o to generate summaries and extract relevant tags
3. Runs multiple AI operations in parallel (cutting processing time in half)
4. Stores everything neatly in your Postgres database
5. Auto-retries when things go wrong (because APIs sometimes fail)
</Steps>

<img src="/analyze_website.svg" alt="AI Web Scraper Workflow" title="AI-powered web scraper workflow DAG" className="responsive-svg" />

## Architecture overview

This tutorial covers building the backend workflow. Future tutorials will add a frontend UI and production deployment.

:::tip[Why pgflow for AI scraping?]
pgflow handles AI workflow complexity: automatic retries for API failures, parallel processing, and full observability.
Everything runs in your Supabase project - no external infrastructure needed.
:::

## Prerequisites

:::caution[Before you begin]

1. Have a Supabase project initialized locally - see [Supabase Local Development Guide](https://supabase.com/docs/guides/local-development)

2. [Install pgflow](/getting-started/install-pgflow/) - it automatically sets up Edge Worker environment variables

3. Add your [OpenAI API key](https://platform.openai.com/api-keys) to `supabase/functions/.env`:
   ```diff
     # supabase/functions/.env
     EDGE_WORKER_DB_URL=postgres://...
     EDGE_WORKER_LOG_LEVEL=info
   + OPENAI_API_KEY=sk-...
   ```
:::

## What you'll learn

<Steps>
1. Write task functions to fetch and process web content
2. Generate structured data from AI using type-safe schemas
3. Create parallel DAG workflows with the TypeScript DSL
4. Compile flows to SQL and apply migrations
5. Execute workflows using the Edge Worker
</Steps>

## Get started

<CardGrid>
  <LinkCard
    title="Part 1: Build the Backend"
    description="Create the database schema, AI tasks, and workflow"
    href="/tutorials/ai-web-scraper/backend/"
  />
</CardGrid>

<JoinCommunity />
