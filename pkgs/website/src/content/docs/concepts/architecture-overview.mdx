---
title: "Architecture Overview"
description: "A high-level introduction to pgflow's architecture"
draft: true
sidebar:
  order: 2
---

pgflow is a Postgres-native workflow engine that uses your database to orchestrate complex processing tasks.

## How Tasks Flow Through The System

At its core, pgflow works through a simple cycle:
1. **SQL Core** decides which tasks are ready and puts them in a queue
2. **Edge Workers** poll the queue and execute the assigned tasks
3. Workers report back success or failure by calling SQL functions
4. SQL Core updates the workflow state and may queue new tasks for dependent steps

This queue-based approach ensures reliable execution while keeping the database as the single source of truth. pgflow is built with three simple layers that work together to make this possible.

## The Three Layers

```mermaid
graph TD
    A[TypeScript DSL] -->|Compiles to| B[SQL Core]
    B -->|Stores in| C[Postgres]
    C <-->|Poll & Complete| D[Edge Worker]
```

1. **TypeScript DSL**: Define workflows with type safety
2. **SQL Core**: Orchestrate workflows inside Postgres
3. **Edge Worker**: Execute tasks in Supabase Edge Functions

## How It Works

1. **Define** workflow as a DAG of steps in TypeScript
2. **Compile** to SQL migration that creates workflow in database
3. **Start** a run with input data
4. **Execute** tasks via stateless workers that poll for work
5. **Complete** runs automatically when all steps finish

## Core Principles

- **Database is the truth**: All state lives in Postgres tables
- **Workflows are immutable**: Once defined, shapes don't change
- **Automatic orchestration**: Steps start when dependencies complete
- **Idempotent operations**: Safe to retry or reapply definitions
- **Stateless workers**: Can restart or scale without losing work

## Data Flow Example

```mermaid
graph LR
    A[website] --> B[summary]
    A --> C[sentiment]
    B --> D[saveToDb]
    C --> D
```

- Each step gets the original run input and outputs from its dependencies
- When a step completes, its output is stored and dependents are scheduled
- Final run output combines all outputs from leaf steps
