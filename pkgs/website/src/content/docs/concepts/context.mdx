---
title: Context Object
description: Understanding how pgflow provides platform resources to handlers through context
sidebar:
  order: 5
  badge:
    text: NEW!
    variant: tip
---

import { Aside, Tabs, TabItem } from '@astrojs/starlight/components';

The context object is pgflow's mechanism for providing handlers with access to platform resources like database connections, environment variables, and service clients. Instead of each handler creating its own connections, pgflow manages these resources centrally and injects them through a standardized context parameter.

:::note[Future: Custom Resources]
The context system is designed to be extensible. Future versions are planned to support custom resources, allowing you to add your own services (Redis, Stripe, etc.) to the context object while maintaining full type safety.
:::

## Why Context Exists

Before context, handlers faced several challenges:

1. **Resource Management** - Each handler needed to create its own database connections and clients
2. **Configuration Access** - No standardized way to access environment variables
3. **Boilerplate Code** - Repeated connection setup in every handler
4. **Resource Leaks** - Risk of not properly closing connections
5. **Testing Complexity** - Difficult to mock resources for testing

The context object solves these problems by providing pre-configured, managed resources that handlers can use immediately.

## Core Principles

Context follows pgflow's design philosophy:

1. **Zero Configuration** - Resources are ready to use without setup
2. **Type Safety** - Full TypeScript support with proper inference
3. **Pre-configured Resources** - Platform adapter provides established connections
4. **Platform Agnostic** - Context interface can adapt to different platforms
5. **Backward Compatible** - Existing single-parameter handlers continue to work

## How Context Works

When pgflow executes a handler, it passes two parameters:

### Before vs After

<Tabs>
<TabItem label="Before (Global Resources)">
```typescript del="import { sql }" del="import { supabase }"
import { sql } from '../db.js';
import { supabase } from '../supabase-client.js';

async function processUser(input: { userId: string }) {
  const [user] = await sql`SELECT * FROM users WHERE id = ${input.userId}`;
  const apiKey = process.env.EXTERNAL_API_KEY;
  const { data } = await supabase.auth.getUser();

  return { user };
}
```
</TabItem>
<TabItem label="After (Context)">
```typescript ins="ctx: Context" "ctx.sql" "ctx.env" "ctx.supabase"
async function processUser(input: { userId: string }, ctx: Context) {
  const [user] = await ctx.sql`SELECT * FROM users WHERE id = ${input.userId}`;
  const apiKey = ctx.env.EXTERNAL_API_KEY;
  const { data } = await ctx.supabase.auth.getUser();

  return { user };
}
```
</TabItem>
</Tabs>

### Complete Flow Example

```typescript title="Using context in flows" ins="ctx" "ctx.sql" "ctx.env" "ctx.supabase"
const ProcessUserFlow = new Flow<{ userId: string }>({
  slug: 'process_user'
})
  .step({ slug: 'validate' }, async (input, ctx) => {
    const [user] = await ctx.sql`SELECT * FROM users WHERE id = ${input.userId}`;
    const apiKey = ctx.env.EXTERNAL_API_KEY;
    const { data } = await ctx.supabase.auth.getUser();

    // ... process with resources
    return { user };
  });
```

<Aside type="note">
  The context parameter is optional. Handlers that don't need platform resources can omit it for backward compatibility.
</Aside>

## Core Resources

These resources are always available regardless of the platform adapter being used.

### `env`
**Type:** `Record<string, string | undefined>`
**Available:** Always

Environment variables from the runtime environment. The exact variables depend on your platform and deployment configuration.

```typescript "ctx.env"
// Queue handler
async function handler(input, ctx) {
  const apiKey = ctx.env.API_KEY;
  const endpoint = ctx.env.SERVICE_ENDPOINT;
}
```

### `shutdownSignal`
**Type:** `AbortSignal`
**Available:** Always

An AbortSignal that triggers when the worker is shutting down. Use this to gracefully handle long-running operations and ensure clean shutdown.

```typescript "ctx.shutdownSignal"
// Step handler
.step({ slug: 'fetch_data' }, async (input, ctx) => {
  // Automatically cancels if worker shuts down
  const response = await fetch('https://api.example.com/data', {
    signal: ctx.shutdownSignal,
    method: 'POST',
    body: JSON.stringify(input)
  });

  return response.json();
})
```

### `rawMessage`
**Type:** `PgmqMessageRecord<T>`
**Available:** Always

The original message from the pgmq queue, containing metadata like message ID, read count, and enqueued timestamp. Useful for debugging and advanced queue operations.

```typescript
interface PgmqMessageRecord<T> {
  msg_id: number;       // Unique message ID from pgmq
  read_ct: number;      // How many times this message has been read
  enqueued_at: string;  // ISO timestamp when message was enqueued
  vt: string;           // ISO timestamp for visibility timeout
  message: T;           // The actual payload
}
```

```typescript "ctx.rawMessage" ".msg_id" ".read_ct" ".enqueued_at"
async function handler(input, ctx) {
  console.log(`Processing message ${ctx.rawMessage.msg_id}`);
  console.log(`Attempt ${ctx.rawMessage.read_ct} of this message`);
  console.log(`Enqueued at ${ctx.rawMessage.enqueued_at}`);
}
```

### `stepTask`
**Type:** `StepTaskRecord<TFlow>`
**Available:** Flow handlers only

Details about the current step task being executed. This is a strongly-typed record that provides the essential task identification along with the typed input for the specific step. Only available in flow step handlers, not in queue workers.

```typescript
interface StepTaskRecord<TFlow> {
  flow_slug: string;    // Slug identifier of the flow
  run_id: string;       // UUID of the current flow run
  step_slug: string;    // Slug identifier of the current step
  input: StepInput;     // Typed input for this specific step (inferred from flow)
  msg_id: number;       // pgmq message ID
}
```

```typescript "ctx.stepTask" ".step_slug" ".run_id" ".flow_slug" ".msg_id"
const MyFlow = new Flow({ slug: 'my_flow' })
  .step({ slug: 'process' }, async (input, ctx) => {
    console.log(`Executing step: ${ctx.stepTask.step_slug}`);
    console.log(`For run: ${ctx.stepTask.run_id}`);
    console.log(`Flow: ${ctx.stepTask.flow_slug}`);
    console.log(`Message ID: ${ctx.stepTask.msg_id}`);
    // ctx.stepTask.input is the same as the input parameter
  });
```

## Supabase Resources

These resources are available when using the Supabase platform adapter.

### `sql`
**Type:** `postgres.Sql`
**Available:** Supabase platform

A configured PostgreSQL client from the postgres.js library, ready for executing SQL queries against your database.

```typescript "ctx.sql"
// Queue handler
async function handler(input, ctx) {
  const users = await ctx.sql`
    SELECT * FROM users
    WHERE created_at > ${input.since}
  `;
  return { userCount: users.length };
}
```

### `supabase`
**Type:** `SupabaseClient`
**Available:** Supabase platform

Supabase client authenticated with the service role key. This has full database access and bypasses RLS. Since Edge Functions run in a trusted environment, this is the only client you need.

```typescript "ctx.supabase"
// Step handler
.step({ slug: 'process_order' }, async (input, ctx) => {
  const { data, error } = await ctx.supabase
    .from('orders')
    .update({ status: 'processing' })
    .eq('id', input.orderId);
    
  return { order: data };
})
```

```typescript "ctx.supabase"
// Queue handler
async function handler(input, ctx) {
  const { data, error } = await ctx.supabase
    .from('users')
    .update({ verified: true })
    .eq('id', input.userId);
}
```

## Using Destructuring for Cleaner Code

While examples in this documentation use `ctx.` for clarity, destructuring often makes your code more readable:

```typescript ins="{ sql, env, rawMessage }"
// Queue handler - destructure only what you need
async function processOrder(input, { sql, env, rawMessage }) {
  console.log(`Processing order ${rawMessage.msg_id}`);
  
  const [order] = await sql`
    SELECT * FROM orders WHERE id = ${input.orderId}
  `;
  
  await sendNotification(order, env.NOTIFICATION_API_KEY);
}
```

```typescript ins="{ sql, supabase, stepTask }"
// Step handler - clean and focused
.step({ slug: 'sync_data' }, async (input, { sql, supabase, stepTask }) => {
  console.log(`Syncing for run ${stepTask.run_id}`);
  
  const data = await sql`SELECT * FROM pending_sync WHERE user_id = ${input.userId}`;
  
  const synced = await supabase
    .from('synced_data')
    .upsert(data)
    .select();
    
  return { syncedCount: synced.data.length };
})
```

<Aside type="tip">
  Destructuring makes it immediately clear which resources a handler depends on, improving code readability and testability.
</Aside>

## Summary

The context object streamlines handler development by:

- Providing ready-to-use platform resources
- Eliminating connection boilerplate
- Centralizing resource access through platform adapter
- Maintaining type safety throughout
- Supporting gradual migration of existing code

Context embodies pgflow's philosophy of being **robust yet simple** - handlers get powerful capabilities without complexity.
