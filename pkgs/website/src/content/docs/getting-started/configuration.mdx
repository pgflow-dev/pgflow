---
title: Configuration
description: Learn how to configure pgflow with options for retry behavior, timeouts, and flow worker settings. Includes defaults and best practices.
sidebar:
  order: 50
---

import { Aside, Badge, CardGrid, LinkCard } from "@astrojs/starlight/components";

pgflow's configuration is divided into two distinct parts, each serving a different purpose in the workflow lifecycle.

## Configuration Overview

pgflow separates configuration into **task execution** settings (how individual steps are executed and retried) and **worker** settings (how the worker processes the entire flow). Task settings are stored in your database and control retry behavior, while worker settings control concurrency, connections, and batching.

<CardGrid>
  <LinkCard 
    title="Flow & Step Configuration" 
    description="Database-persisted settings that define retry behavior, timeouts, and error handling. Can be updated on the fly without redeployment using SQL queries."
    href="#flow--step-configuration"
  />
  <LinkCard 
    title="Worker Configuration" 
    description="Runtime settings that control how EdgeWorker executes tasks. These affect concurrency, polling behavior, and resource usage. Requires redeployment to change."
    href="#worker-configuration"
  />
</CardGrid>

<Aside title="Zero Configuration" type="tip">
pgflow comes with sensible defaults for all configuration options.

You only need to specify slugs for your flow and steps:

```ts
new Flow({ slug: 'my_flow' })
  .step({ slug: 'step1' }, handler1)
  .step({ slug: 'step2' }, handler2)
```
</Aside>

## Flow & Step Configuration

These settings are defined in your TypeScript flow code and compiled into SQL migrations. They control how individual tasks behave when they fail or timeout.

<Aside type="tip">
After deployment, you can update these settings without recompiling your flow. See [Update Flow Options](/how-to/update-flow-options/) for details.
</Aside>

### Configuration Options

#### Shared Options (Flow & Step)

These options can be set at both flow and step levels. Step-level settings override flow defaults.

##### `maxAttempts`
**Type:** `number`  
**Default:** `3`  
**Levels:** Flow & Step

The maximum number of times a task will be attempted before being marked as permanently failed.

```ts
// Flow level
new Flow({ slug: 'my_flow', maxAttempts: 5 })

// Step level (overrides flow default)
.step({ slug: 'my_step', maxAttempts: 7 }, handler)
```

##### `baseDelay`
**Type:** `number`  
**Default:** `5`  
**Levels:** Flow & Step

The initial delay (in seconds) before the first retry. pgflow uses exponential backoff, so subsequent retries will have increasingly longer delays.

```ts
// Flow level
new Flow({ slug: 'my_flow', baseDelay: 2 })

// Step level (overrides flow default)
.step({ slug: 'my_step', baseDelay: 10 }, handler)
```

##### `timeout`
**Type:** `number`  
**Default:** `60`  
**Levels:** Flow & Step

The visibility timeout (in seconds) - how long a task remains invisible to other workers while being processed.

<Aside type="caution" title="Timeout and Task Processing">
Set `timeout` higher than your task's maximum processing time.
  <details>
  <summary>
  Here's why:
  </summary>
  - When a worker picks up a task, it becomes invisible for `timeout` seconds
  - If processing takes longer than `timeout`, the task becomes visible again
  - Other workers can then pick up and process the same task
  - This leads to duplicate processing
  - For example: with `timeout: 30` and a task that takes 45 seconds, the task could be processed twice
  </details>

Currently, pgflow uses timeout only for visibility. In the future, the Edge Worker will also use it to terminate tasks that exceed their timeout.
</Aside>

```ts
// Flow level
new Flow({ slug: 'my_flow', timeout: 120 })

// Step level (overrides flow default)
.step({ slug: 'my_step', timeout: 300 }, handler)
```

#### Step-Only Options

These options can only be set at the step level.

<Aside type="note">
Step-only options are coming soon. The first will be `startDelay` for delaying initial task execution.
</Aside>

### Setting Configuration

#### Flow-level defaults

Set default configuration for all steps in your flow:

```typescript
new Flow({
  slug: 'my_flow',
  maxAttempts: 3,    // Default for all steps
  baseDelay: 5,      // Default for all steps
  timeout: 60        // Default for all steps
})
```

#### Step-level overrides

Override flow defaults for specific steps that need different behavior:

```typescript
new Flow({
  slug: 'analyze_data',
  maxAttempts: 3,    // Flow defaults
  baseDelay: 5,
  timeout: 60
})
  .step({
    slug: 'fetch_data',
    // Uses all flow defaults
  }, fetchHandler)
  .step({
    slug: 'process_data',
    maxAttempts: 5,    // Override: more retries
    timeout: 300       // Override: needs more time
    // baseDelay uses flow default (5)
  }, processHandler)
  .step({
    slug: 'call_api',
    baseDelay: 10,     // Override: longer initial delay
    // maxAttempts and timeout use flow defaults
  }, apiHandler)
```

### Retry Behavior

pgflow uses **exponential backoff** for retries. The delay between attempts is calculated as:

```
delay = baseDelay * 2^attemptCount
```

<Aside type="note">
Unlike the Edge Worker's queue-only mode which supports a `maxDelay` cap, pgflow's retry delays are not capped yet. Delays will continue to double with each attempt.
</Aside>

### Retry Delay Examples

Here's how retry delays grow with different base delays:

| Attempt | Delay (baseDelay: 2s) | Delay (baseDelay: 5s) | Delay (baseDelay: 10s) |
|---------|----------------------|----------------------|------------------------|
| 1       | 2s                   | 5s                   | 10s                    |
| 2       | 4s                   | 10s                  | 20s                    |
| 3       | 8s                   | 20s                  | 40s                    |
| 4       | 16s                  | 40s                  | 80s                    |
| 5       | 32s                  | 80s                  | 160s                   |
| 6       | 64s                  | 160s                 | 320s                   |
| 7       | 128s                 | 320s                 | 640s                   |

### When Tasks Fail Permanently

A task is marked as permanently failed when:
- It has been attempted `maxAttempts` times
- Each attempt resulted in an error
- The task status changes from `queued` to `failed`
- The error message from the last attempt is stored

## Worker Configuration

These settings control how the EdgeWorker processes tasks from your flows. They're passed when starting the worker and affect performance and resource utilization.

<Aside type="note">
Worker configuration requires redeploying your Edge Function to change, while flow/step configuration can be updated on the fly with SQL queries. See [Update Flow Options](/how-to/update-flow-options/) for details on updating configuration without redeployment.
</Aside>

### Default Configuration

When using pgflow with EdgeWorker, configure the worker's behavior:

```typescript
import { EdgeWorker } from '@pgflow/edge-worker';
import MyFlow from './flows/my_flow.ts';

EdgeWorker.start(MyFlow, {
  // Worker concurrency settings
  maxConcurrent: 10,      // Process up to 10 tasks simultaneously
  maxPgConnections: 4,    // Database connection pool size
  
  // Polling configuration
  batchSize: 10,          // Fetch up to 10 tasks per poll
  maxPollSeconds: 2,      // Long-poll duration
  pollIntervalMs: 100,    // Database polling frequency
});
```

### `maxConcurrent`
**Type:** `number`  
**Default:** `10`

Maximum number of tasks that can be processed simultaneously. Increase for I/O-heavy tasks, decrease for CPU-heavy tasks.

### `maxPgConnections`
**Type:** `number`  
**Default:** `4`

Size of the PostgreSQL connection pool. Should generally be less than `maxConcurrent` since not all tasks need a connection at all times.

### `batchSize`
**Type:** `number`  
**Default:** `10`

Maximum number of tasks to fetch in a single poll operation. Larger batches are more efficient but may lead to uneven work distribution.

### `maxPollSeconds`
**Type:** `number`  
**Default:** `2`

How long to wait for tasks during each poll cycle. Shorter durations ensure more responsive task pickup.

<Aside type="caution">
Keep `maxPollSeconds` at 2 seconds or lower for flow workers to ensure proper heartbeat intervals.
</Aside>

### `pollIntervalMs`
**Type:** `number`  
**Default:** `100`

How frequently (in milliseconds) to check the database for new tasks during the poll cycle.

## Complete Examples

These examples show how flow/step configuration and worker configuration work together in real scenarios.

### High-Reliability API Integration

For flows that interact with external APIs that may be temporarily unavailable:

```typescript
new Flow({
  slug: 'sync_external_data',
  maxAttempts: 7,     // More retries for transient failures
  baseDelay: 10,      // Start with 10s, reaching ~20 minutes by attempt 7
  timeout: 300        // 5 minutes for slow APIs
})
```

### Fast Local Processing

For flows that process data locally and should fail fast:

```typescript
new Flow({
  slug: 'process_upload',
  maxAttempts: 2,     // Fail fast - only 1 retry
  baseDelay: 1,       // Minimal delay between attempts
  timeout: 30         // Quick operations
})
```

### Mixed Workload

When different steps have different reliability requirements:

```typescript
new Flow({
  slug: 'data_pipeline',
  maxAttempts: 3,     // Sensible defaults
  baseDelay: 5,
  timeout: 60
})
  .step({
    slug: 'validate_input',
    maxAttempts: 1,   // No retries - validation should not fail
    timeout: 10       // Very quick
  }, validateHandler)
  .step({
    slug: 'fetch_external',
    maxAttempts: 5,   // External API might be flaky
    baseDelay: 10,    // Longer delays for external service
    timeout: 120      // Allow more time
  }, fetchHandler)
  .step({
    slug: 'save_results',
    // Use flow defaults
  }, saveHandler)
```