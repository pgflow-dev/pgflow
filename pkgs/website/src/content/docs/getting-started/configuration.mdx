---
title: Configuration
description: Learn how to configure pgflow with options for retry behavior, timeouts, and flow worker settings. Includes defaults and best practices.
sidebar:
  order: 50
---

import { Aside, Badge } from "@astrojs/starlight/components";

You can configure pgflow at both the flow and step levels to control retry behavior, timeouts, and execution settings.

<Aside title="Zero Configuration" type="tip">
pgflow comes with sensible defaults for all configuration options.

You only need to specify slugs for your flow and steps:

```ts
new Flow({ slug: 'my_flow' })
  .step({ slug: 'step1' }, handler1)
  .step({ slug: 'step2' }, handler2)
```
</Aside>

## Default configuration

```typescript
// Flow-level configuration
new Flow({
  slug: 'my_flow',
  
  // Retry configuration
  maxAttempts: 3,    // max retry attempts before marking as failed
  baseDelay: 5,      // initial retry delay in seconds
  
  // Execution configuration  
  timeout: 60,       // visibility timeout in seconds
})

// Step-level configuration (overrides flow defaults)
.step({
  slug: 'my_step',
  maxAttempts: 5,    // override flow's maxAttempts
  baseDelay: 2,      // override flow's baseDelay
  timeout: 30,       // override flow's timeout
}, handler)
```

## Flow Configuration

### `maxAttempts`
**Type:** `number`  
**Default:** `3`

The maximum number of times a task will be attempted before being marked as permanently failed.

```ts
new Flow({
  slug: 'my_flow',
  maxAttempts: 5  // Try up to 5 times before giving up
})
```

### `baseDelay`
**Type:** `number`  
**Default:** `5`

The initial delay (in seconds) before the first retry. pgflow uses exponential backoff, so subsequent retries will have increasingly longer delays.

```ts
new Flow({
  slug: 'my_flow',
  baseDelay: 2  // Start with 2 second delay
})
```

### `timeout`
**Type:** `number`  
**Default:** `60`

The visibility timeout (in seconds) - how long a task remains invisible to other workers while being processed.

<Aside type="caution" title="Timeout and Task Processing">
Set `timeout` higher than your task's maximum processing time.
  <details>
  <summary>
  Here's why:
  </summary>
  - When a worker picks up a task, it becomes invisible for `timeout` seconds
  - If processing takes longer than `timeout`, the task becomes visible again
  - Other workers can then pick up and process the same task
  - This leads to duplicate processing
  - For example: with `timeout: 30` and a task that takes 45 seconds, the task could be processed twice
  </details>

Currently, pgflow uses timeout only for visibility. In the future, the Edge Worker will also use it to terminate tasks that exceed their timeout.
</Aside>

```ts
new Flow({
  slug: 'my_flow',
  timeout: 120  // Tasks invisible for 2 minutes while processing
})
```

## Step Configuration

Individual steps can override flow-level defaults by providing their own configuration:

```typescript
new Flow({
  slug: 'analyze_data',
  maxAttempts: 3,    // Flow default
  baseDelay: 5,      // Flow default  
  timeout: 60        // Flow default
})
  .step({
    slug: 'fetch_data',
    // Uses flow defaults
  }, async (input) => {
    // Quick operation, flow defaults are fine
    return await fetchData(input.run.source);
  })
  .step({
    slug: 'process_data',
    maxAttempts: 5,    // Override: more retries for flaky processing
    timeout: 300       // Override: needs more time
  }, async (input) => {
    // Long-running operation that may fail
    return await processLargeDataset(input.fetch_data);
  })
```

## Retry Behavior

pgflow uses **exponential backoff** for retries. The delay between attempts is calculated as:

```
delay = baseDelay * 2^attemptCount
```

<Aside type="note">
Unlike the Edge Worker's queue-only mode which supports a `maxDelay` cap, pgflow's retry delays are not capped yet. Delays will continue to double with each attempt.
</Aside>

### Retry Delay Examples

Here's how retry delays grow with different base delays:

| Attempt | Delay (baseDelay: 2s) | Delay (baseDelay: 5s) | Delay (baseDelay: 10s) |
|---------|----------------------|----------------------|------------------------|
| 1       | 2s                   | 5s                   | 10s                    |
| 2       | 4s                   | 10s                  | 20s                    |
| 3       | 8s                   | 20s                  | 40s                    |
| 4       | 16s                  | 40s                  | 80s                    |
| 5       | 32s                  | 80s                  | 160s                   |
| 6       | 64s                  | 160s                 | 320s                   |
| 7       | 128s                 | 320s                 | 640s                   |

### When Tasks Fail Permanently

A task is marked as permanently failed when:
- It has been attempted `maxAttempts` times
- Each attempt resulted in an error
- The task status changes from `queued` to `failed`
- The error message from the last attempt is stored

## Flow Worker Configuration

When using pgflow with EdgeWorker, you can configure the worker's behavior:

```typescript
import { EdgeWorker } from '@pgflow/edge-worker';
import MyFlow from './flows/my_flow.ts';

EdgeWorker.start(MyFlow, {
  // Worker concurrency settings
  maxConcurrent: 10,      // Process up to 10 tasks simultaneously
  maxPgConnections: 4,    // Database connection pool size
  
  // Polling configuration
  batchSize: 10,          // Fetch up to 10 tasks per poll
  maxPollSeconds: 2,      // Long-poll duration
  pollIntervalMs: 100,    // Database polling frequency
});
```

### Worker Options

#### `maxConcurrent`
**Type:** `number`  
**Default:** `10`

Maximum number of tasks that can be processed simultaneously. Increase for I/O-heavy tasks, decrease for CPU-heavy tasks.

#### `maxPgConnections`
**Type:** `number`  
**Default:** `4`

Size of the PostgreSQL connection pool. Should generally be less than `maxConcurrent` since not all tasks need a connection at all times.

#### `batchSize`
**Type:** `number`  
**Default:** `10`

Maximum number of tasks to fetch in a single poll operation. Larger batches are more efficient but may lead to uneven work distribution.

#### `maxPollSeconds`
**Type:** `number`  
**Default:** `2`

How long to wait for tasks during each poll cycle. Shorter durations ensure more responsive task pickup.

<Aside type="caution">
Keep `maxPollSeconds` at 2 seconds or lower for flow workers to ensure proper heartbeat intervals.
</Aside>

#### `pollIntervalMs`
**Type:** `number`  
**Default:** `100`

How frequently (in milliseconds) to check the database for new tasks during the poll cycle.

## Configuration Examples

### High-Reliability API Integration

For flows that interact with external APIs that may be temporarily unavailable:

```typescript
new Flow({
  slug: 'sync_external_data',
  maxAttempts: 7,     // More retries for transient failures
  baseDelay: 10,      // Start with 10s, reaching ~20 minutes by attempt 7
  timeout: 300        // 5 minutes for slow APIs
})
```

### Fast Local Processing

For flows that process data locally and should fail fast:

```typescript
new Flow({
  slug: 'process_upload',
  maxAttempts: 2,     // Fail fast - only 1 retry
  baseDelay: 1,       // Minimal delay between attempts
  timeout: 30         // Quick operations
})
```

### Mixed Workload

When different steps have different reliability requirements:

```typescript
new Flow({
  slug: 'data_pipeline',
  maxAttempts: 3,     // Sensible defaults
  baseDelay: 5,
  timeout: 60
})
  .step({
    slug: 'validate_input',
    maxAttempts: 1,   // No retries - validation should not fail
    timeout: 10       // Very quick
  }, validateHandler)
  .step({
    slug: 'fetch_external',
    maxAttempts: 5,   // External API might be flaky
    baseDelay: 10,    // Longer delays for external service
    timeout: 120      // Allow more time
  }, fetchHandler)
  .step({
    slug: 'save_results',
    // Use flow defaults
  }, saveHandler)
```