---
title: Scraping a website with pgflow and AI
---

import { Steps } from '@astrojs/starlight/components';

**Read:** ~15 min / **Build:** ~30 min

## What we will build

A resilient workflow that combines web scraping with OpenAI's powerful text analysis capabilities. You'll create a workflow with 4 tasks (exposed as 4 pgflow steps) that:

1. Fetches a web page content
2. Processes it with **two parallel AI operations** (summary and keyword extraction)
3. Writes everything into your Postgres database

![Analyze Website Workflow](/analyze_website.svg "Analyze website workflow DAG")

The diagram illustrates this workflow.

This tutorial will guide you through building a website scraper that analyzes content with AI and stores results in your database.

:::tip[Why pgflow?]
Unlike traditional solutions that require separate databases and job queues, pgflow stores the entire workflow DAG, retry logic, and execution history directly inside Postgres. While pgflow does require an Edge Worker to process tasks, it's included in the package. This architecture means simpler infrastructure, stronger consistency guarantees, and built-in observability.
:::

## Prerequisites

:::caution[Before you begin]

1. Follow the [install pgflow guide](/getting-started/install-pgflow/) which covers all required steps, versions, and setup instructions.

2. Get an OpenAI API key from the [OpenAI Platform](https://platform.openai.com/api-keys) and add it to your environment:

```diff
  # supabase/functions/.env
  EDGE_WORKER_DB_URL=postgres://...
  EDGE_WORKER_LOG_LEVEL=info
+ OPENAI_API_KEY=sk-...
```
:::

---

## Step 1 — Create `websites` table

First, create a migration file to set up the database table where the website analysis results will be stored.

### Create a new migration

Create a new migration file in your Supabase project:

```bash
npx supabase migration new add_websites
```

### Add the table definition

Open the newly created migration file and add this SQL:

```sql
-- Create the websites table to store analysis results
create table public.websites (
  id           bigserial primary key,
  website_url  text not null,
  summary      text,
  tags         text[],
  created_at   timestamptz default now()
);
```

This table provides everything needed: a unique ID, the website URL, fields for AI-generated content, and a timestamp for when the record was created.

### Apply the migration

Run the migration to create the table in your local database:

```bash
npx supabase migrations up --local

# Verify the table was created with:
select * from public.websites;
```

---

## Step 2 — Create task functions

One of pgflow's most powerful features is how it breaks complex workflows into smaller, independently retriable steps. Let's create four specialized task functions:

Create these four task functions in `supabase/functions/_tasks`:
- `scrapeWebsite.ts` - Fetches webpage content
- `summarize.ts` - Generates AI summary
- `extractTags.ts` - Extracts keyword tags
- `saveWebsite.ts` - Saves to database

### Web Scraping Function

This function fetches a webpage and strips out HTML tags to create clean text for AI analysis:

<details open>
  <summary><strong>scrapeWebsite.ts</strong> - Fetch and clean webpage content</summary>

```ts
# supabase/functions/_tasks/scrapeWebsite.ts
export default async function scrapeWebsite(url: string) {
  console.log("[scrapeWebsite] fetching", url);

  // Add timeout to prevent requests from hanging (10 seconds)
  const res = await fetch(url, {
    signal: AbortSignal.timeout(10000)
  });

  if (!res.ok) throw new Error(`Fetch failed: ${res.status}`);

  const html = await res.text();
  const text = html.replace(/<[^>]+>/g, " ").replace(/\s+/g, " ").trim();

  // Return just the content, limited to a reasonable size
  return { content: text.slice(0, 25_000) };   // keep token count sane
}
```

:::caution[Production HTML Processing]
In production, use html-to-text for full HTML parsing.
:::
</details>

### AI Analysis Functions

Two OpenAI-powered functions analyze the webpage content:

:::tip[OpenAI Responses API with Structured Outputs]
These functions use OpenAI's newer [Responses API](https://platform.openai.com/docs/api-reference/responses) with [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs) to:
- Get type-safe, validated responses directly from the AI model
- Process content in a predictable format through schema validation
- Simplify error handling with automatic validation
:::

<details>
  <summary><strong>summarize.ts</strong> - Generate an AI summary</summary>

Generates a concise summary paragraph from the webpage content:

```ts
# supabase/functions/_tasks/summarize.ts
import OpenAI from "jsr:@openai/openai@4.33.0";
import { z } from "jsr:@zod/zod";
import { zodTextFormat } from "jsr:@openai/openai@4.33.0/helpers/zod";

const openai = new OpenAI();
const SummarySchema = z.object({ summary: z.string() });

export default async function summarize(content: string) {
  console.log("[summarize] processing content");

  const rsp = await openai.responses.parse({
    model: "gpt-4o",
    input: [
      { role: "system", content: "Return a short paragraph summary." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(SummarySchema, "summary") },
  });

  return rsp.output_parsed.summary;
}
```
</details>

<details>
  <summary><strong>extractTags.ts</strong> - Extract keyword tags</summary>

Extracts 5-10 relevant keyword tags from the content:

```ts
# supabase/functions/_tasks/extractTags.ts
import OpenAI from "jsr:@openai/openai@4.33.0";
import { z } from "jsr:@zod/zod";
import { zodTextFormat } from "jsr:@openai/openai@4.33.0/helpers/zod";

const openai = new OpenAI();
const TagsSchema = z.object({ keywords: z.array(z.string()).max(10) });

export default async function extractTags(content: string) {
  console.log("[extractTags] extracting keywords");

  const rsp = await openai.responses.parse({
    model: "gpt-4o",
    input: [
      { role: "system",
        content: "Return 5-10 descriptive keywords." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(TagsSchema, "keywords") },
  });

  return rsp.output_parsed.keywords;
}
```
</details>

### Save to Database Function

The final task persists all the analysis results to the database table created in Step 1.

<details>
  <summary><strong>saveWebsite.ts</strong> - Store results in the database</summary>

This function connects to Supabase using environment variables and inserts the website data, AI summary, and tags into the database.

```ts
import { createClient } from "@supabase/supabase-js";

const supabase = createClient(
  Deno.env.get("SUPABASE_URL")!,
  Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!,
);

export default async function saveWebsite(row: {
  website_url: string; summary: string; tags: string[];
}) {
  console.log("[saveWebsite] inserting row");

  const { data } = await supabase
    .from("websites")
    .insert(row)
    .select("*")
    .single()
    .throwOnError();

  console.log("[saveWebsite] inserted with id:", data?.id);

  return data;
}
```

:::note
This function requires the service role key (not the anon key) because it directly inserts data without RLS policies. Both the URL and service role key are automatically available in the Edge Function environment.
:::
</details>

<details>
  <summary>Why separate files instead of one big module?</summary>

  Breaking the workflow into discrete functions provides important benefits:

  1. **Selective retries** - If the OpenAI API rate-limits us, pgflow can retry just the failed AI call without re-scraping the website
  2. **Parallelization** - As you'll see later, the summary and tag extraction can run concurrently
  3. **Clearer error handling** - When something fails, you know exactly which step had the problem
  4. **Better testability** - Each function can be tested in isolation
</details>

---

## Step 3 — Define flow

Now comes the magic - connecting these tasks together using pgflow's TypeScript DSL. This creates a directed acyclic graph (DAG) that explicitly shows the data dependencies between steps.

```ts title="_flows/analyze_website.ts"
import { Flow } from "@pgflow/dsl";
import scrapeWebsite   from "../_tasks/scrapeWebsite.ts";
import summarize from "../_tasks/summarize.ts";
import extractTags     from "../_tasks/extractTags.ts";
import saveWebsite     from "../_tasks/saveWebsite.ts";

type Input = { url: string };

export default new Flow<Input>({ slug: "analyze_website", maxAttempts: 3 })
  .step({ slug: "website" },
    ({ run })      => scrapeWebsite(run.url))

  .step({ slug: "summary", dependsOn: ["website"] },
    ({ website })  => summarize(website.content))

  .step({ slug: "tags",    dependsOn: ["website"] },
    ({ website })  => extractTags(website.content))

  .step({ slug: "saveToDb", dependsOn: ["summary", "tags"] },
    ({ run, summary, tags }) =>
      saveWebsite({ website_url: run.url, summary, tags }));
```

The flow dependencies at a glance:
- `website` is the root step (no dependencies)
- `summary` depends on `website`
- `tags` depends on `website`
- `saveToDb` depends on both `summary` and `tags`

Breaking down this flow definition:

1. First comes the flow's input type definition, requiring a URL and user ID
2. A **root step** scrapes the website, taking input from the flow's parameters
3. Two **parallel steps** (`summary` and `tags`) both depend on the website content
4. A final step waits for both AI operations to complete, then saves everything to the database

:::tip[Automatic Parallelization]
pgflow automatically runs the summary and tags steps in parallel since they only depend on the website content and not on each other. This cuts execution time nearly in half and isolates failures - if one AI call fails, only that specific step retries.
:::

:::note
The `maxAttempts: 3` option in the flow definition affects all steps. Each step will automatically retry up to 3 times on failure before being marked as permanently failed.
:::

For a deeper understanding of flow definitions, see the [Flow DSL guide](/concepts/flow-dsl/).

---

## Step 4 — Compile & migrate

Unlike many workflow systems that interpret definitions at runtime, pgflow compiles your TypeScript flow into native SQL that Postgres can directly execute. This gives you the safety of type checking with the performance of native database execution.

Let's compile our flow and add it to the database:

```bash
# turn the TypeScript DAG into SQL
npx pgflow@latest compile supabase/functions/_flows/analyze_website.ts

# apply the generated migration to local Postgres
npx supabase migrations up --local
```

This creates a migration file (like `supabase/migrations/pgflow_2024...sql`) containing all the SQL needed to register your flow in the pgflow system.

:::danger[Adding or removing steps]
When adding or removing steps, you must create a new flow. See [versioning your flows](/how-to/version-your-flows/) for details. This limitation is temporary.
:::

---

## Step 5 — Create worker

For your flow to run, you need to create an Edge Worker:

```bash
# Create a worker for your flow
npx supabase functions new analyze_website_worker
```

Replace the contents of the generated index.ts with:

```typescript title="supabase/functions/analyze_website_worker/index.ts"
import { EdgeWorker } from "jsr:@pgflow/edge-worker";
import AnalyzeWebsite from '../_flows/analyze_website.ts';

// Pass the flow definition to the Edge Worker
EdgeWorker.start(AnalyzeWebsite);
```

Disable JWT verification in `supabase/config.toml`:

```diff title="supabase/config.toml"
  [functions.analyze_website_worker]
  enabled = true
- verify_jwt = true
+ verify_jwt = false
```

:::note
This JWT verification is disabled for local development only. In production, you would typically keep JWT verification enabled and start the Edge Worker with a token. For more details on Edge Worker setup and configuration, see the [Run Flow guide](/getting-started/run-flow/).
:::

---

## Step 6 — Run & test

Now it's time to see our creation in action. We'll need to run three commands:

<Steps>
1. Start Supabase and local services:
   ```bash frame="none"
   npx supabase start
   ```

2. Serve the Edge Functions:
   ```bash frame="none"
   npx supabase functions serve
   ```

3. Start the Edge Worker:
   ```bash frame="none"
   curl http://127.0.0.1:54321/functions/v1/analyze_website_worker
   ```
   You should get `ok` in the response; keep this terminal open.
</Steps>

With everything running, let's trigger our flow from the Supabase SQL editor (or `psql`):

```sql frame="terminal" title="Run this in Supabase SQL Editor"
select * from pgflow.start_flow(
  flow_slug => 'analyze_website',
  input     => '{"url":"https://supabase.com"}'
);
```

:::note[Execution Flow]
The Edge Worker executes `scrapeWebsite` first, then runs both AI steps in parallel, and finally saves the results once all analysis is complete. The database maintains a complete execution record.
:::

### What you should see

In Terminal 2 (functions serve), you'll see the progress logs:

```
[scrapeWebsite] fetching https://supabase.com
[summarize] processing content
[extractTags] extracting keywords
[saveWebsite] inserting row
[saveWebsite] inserted with id: 1
```

And in your database, you'll find your new record:

```sql
select website_url, tags, left(summary,60)||'...' as summary
from websites order by id desc limit 1;
```

```
| website_url           | tags                                   | summary                              |
|-----------------------|----------------------------------------|--------------------------------------|
| https://supabase.com  | {postgres,serverless,open-source,...}  | Supabase is an open-source Firebase  |
```

---

## What you've built

Congratulations! You've built a sophisticated ETL pipeline with powerful features:

- **Resilient processing** - Each step automatically retries on failure (up to 3 times per our configuration)
- **Parallel execution** - When steps don't depend on each other, they run concurrently
- **Complete audit trail** - Every run's execution history is preserved in your database
- **Modular design** - Each processing step is isolated and independently testable

And you've done all this with under 200 lines of code, without any additional infrastructure beyond your Postgres database!

---

## Troubleshooting

<details>
  <summary>Common issues and solutions</summary>

| Symptom | Fix |
|---------|-----|
| `Missing OPENAI_API_KEY` | Export the env var in `supabase/.env` and restart `functions serve`. Check console for error: `Error: Request failed with status code 401 (Unauthorized)` |
| Workers not running | Ensure you've run the compilation step and applied migrations correctly. |
| Row not created | Check `SUPABASE_SERVICE_ROLE_KEY` – needs *service-role* level. |
| `TypeError: fetch failed` | Check your internet connection. Docker containers need network access to reach OpenAI API. |
</details>

For deeper debugging help, see the [Monitor flow execution](/how-to/monitor-flow-execution/) guide.
