---
title: Scraping a website with pgflow and AI
---

Ever wanted to automatically scrape a website, analyze its content with AI, and store the results in your database? In this tutorial, you'll build exactly that: a resilient workflow that combines web scraping with OpenAI's powerful text analysis capabilities.

You'll create a 4-step workflow that fetches a web page, **processes it with two parallel AI operations** (summary and keyword extraction), and writes everything into your Postgres database. By the end, you'll see both real-time console logs from each step and a neatly stored row in `public.websites`.

![Analyze Website Workflow](/analyze_website.svg)

The diagram illustrates this workflow. Notice how the AI operations run in parallel after the website content is fetched, significantly reducing the total execution time.

:::tip[Why pgflow?]
Unlike traditional approaches that require separate job queues and processing infrastructure, pgflow stores the entire workflow DAG, retry logic, and execution history directly inside Postgres. This means simpler infrastructure, stronger consistency guarantees, and built-in observability.
:::

## Prerequisites

:::caution[Required setup]
Before you begin, make sure you have:
- A local Supabase project with pgflow installed
- If you haven't set up pgflow yet, follow the [install pgflow guide](/getting-started/install-pgflow/) first
- If you need to create a Supabase project, see the [getting started guide](/getting-started/) first
:::

:::caution[Required OpenAI API Key]
The AI analysis steps in this workflow **require a valid OpenAI API key**. Make sure to:

1. Sign up for an API key at https://platform.openai.com if you don't have one
2. Add your key to `supabase/functions/.env` alongside existing Edge Worker variables:

```diff
# supabase/functions/.env
  EDGE_WORKER_DB_URL=postgres://...
  EDGE_WORKER_LOG_LEVEL=info
+ # OpenAI API key for AI analysis steps
+ OPENAI_API_KEY=sk-...
```

Without this key, the AI analysis steps will fail with authentication errors.
:::

---

## Step 1 - Create the destination table

First, create a migration file to set up the database table where the website analysis results will be stored.

### Create a new migration

Create a new migration file in your Supabase project:

```bash
npx supabase migration new add_websites
```

This will create a new file in the `supabase/migrations` directory with a timestamp prefix, like `20240516123456_add_websites.sql`.

### Add the table definition

Open the newly created migration file and add this SQL:

```sql
-- Create the websites table to store analysis results
create table public.websites (
  id           bigserial primary key,
  user_id      uuid,
  website_url  text not null,
  summary      text,
  tags         text[],
  created_at   timestamptz default now()
);

-- Add index to speed up common queries
create index idx_websites_user_id on public.websites (user_id);
```

This table provides everything needed: a unique ID, the user who initiated the analysis, the website URL, fields for AI-generated content, and a timestamp for when the record was created.

### Apply the migration

Run the migration to create the table in your local database:

```bash
npx supabase migration up --local
```

You should see confirmation that the migration was applied successfully. You can verify the table was created by checking the Supabase Dashboard's Table Editor or running `select * from public.websites;` in the SQL Editor.

---

## Step 2 - Create modular task functions

One of pgflow's most powerful features is how it breaks complex workflows into smaller, independently retriable steps. Let's create four specialized task functions:

| Function | Purpose |
|---------|---------|
| `scrapeWebsite.ts` | Fetch and extract plain text from a URL |
| `summarizeWithAI.ts` | Generate a concise text summary using OpenAI |
| `extractTags.ts` | Extract keyword tags using OpenAI |
| `saveWebsite.ts` | Store results in the database |

Create these files in your `supabase/functions/_tasks` directory.

### Web Scraper Function

This function fetches a webpage, strips out HTML tags, and returns clean text for the AI to analyze. The 25,000 character limit prevents oversized API calls to OpenAI.

<details open>
  <summary><strong>scrapeWebsite.ts</strong> - Fetch and clean webpage content</summary>

```ts
export default async function scrapeWebsite(url: string) {
  console.log("[scrapeWebsite] fetching", url);

  const res = await fetch(url);
  if (!res.ok) throw new Error(`Fetch failed: ${res.status}`);

  const html = await res.text();
  const text = html.replace(/<[^>]+>/g, " ").replace(/\s+/g, " ").trim();

  // Return just the content, limited to a reasonable size
  return { content: text.slice(0, 25_000) };   // keep token count sane
}
```

:::caution[Production HTML Processing]
The HTML sanitization here (using regex) is intentionally simplified for the tutorial. For production use, consider using a proper HTML-to-text library like [html-to-text](https://github.com/html-to-text/node-html-to-text) or [Turndown](https://github.com/mixmark-io/turndown) for more robust HTML-to-markdown conversion.
:::
</details>

### AI Analysis Functions

These two functions use OpenAI's API to analyze the webpage content in different ways. They both use Zod schemas to validate the AI responses and ensure proper typing.

<details>
  <summary><strong>summarizeWithAI.ts</strong> - Generate an AI summary</summary>

This function asks GPT-4o to generate a concise summary of the webpage content, returning a single paragraph of text.

```ts
import OpenAI from "openai";
import { z } from "zod";
import { zodTextFormat } from "openai/helpers/zod";

const openai = new OpenAI();
const Shape = z.object({ summary: z.string() });

export default async function summarizeWithAI(content: string) {
  console.log("[summarizeWithAI] len", content.length);

  const rsp = await openai.responses.parse({
    model: "gpt-4o-2024-08-06",
    input: [
      { role: "system", content: "Return a short paragraph summary." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(Shape, "summary") },
  });

  return rsp.output_parsed.summary;
}
```
</details>

<details>
  <summary><strong>extractTags.ts</strong> - Extract keyword tags</summary>

This function analyzes the content to extract 5-10 relevant keyword tags that describe the webpage's topics or themes.

```ts
import OpenAI from "openai";
import { z } from "zod";
import { zodTextFormat } from "openai/helpers/zod";

const openai = new OpenAI();
const Tags = z.object({ keywords: z.array(z.string()).max(10) });

export default async function extractTags(content: string) {
  console.log("[extractTags] run");

  const rsp = await openai.responses.parse({
    model: "gpt-4o-2024-08-06",
    input: [
      { role: "system",
        content: "Return 5-10 descriptive keywords." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(Tags, "tags") },
  });

  return rsp.output_parsed.keywords;
}
```
</details>

### Database Function

The final task persists all the analysis results to the database table created in Step 1.

<details>
  <summary><strong>saveWebsite.ts</strong> - Store results in the database</summary>

This function connects to Supabase using environment variables and inserts the website data, AI summary, and tags into the database.

```ts
import { createClient } from "@supabase/supabase-js";

const supabase = createClient(
  Deno.env.get("SUPABASE_URL")!,
  Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!,
);

export default async function saveWebsite(row: {
  user_id: string; website_url: string; summary: string; tags: string[];
}) {
  console.log("[saveWebsite] inserting row");
  await supabase.from("websites").insert(row).throwOnError();
}
```
</details>

<details>
  <summary>Why separate files instead of one big module?</summary>

  Breaking the workflow into discrete functions provides important benefits:

  1. **Selective retries** - If the OpenAI API rate-limits us, pgflow can retry just the failed AI call without re-scraping the website
  2. **Parallelization** - As you'll see later, the summary and tag extraction can run concurrently
  3. **Clearer error handling** - When something fails, you know exactly which step had the problem
  4. **Better testability** - Each function can be tested in isolation
</details>

---

## Step 3 - Connect everything with a flow definition

Now comes the magic - connecting these tasks together using pgflow's TypeScript DSL. This creates a directed acyclic graph (DAG) that explicitly shows the data dependencies between steps.

```ts title="_flows/analyze_website.ts"
import { Flow } from "@pgflow/dsl";
import scrapeWebsite   from "../_tasks/scrapeWebsite.ts";
import summarizeWithAI from "../_tasks/summarizeWithAI.ts";
import extractTags     from "../_tasks/extractTags.ts";
import saveWebsite     from "../_tasks/saveWebsite.ts";

type Input = { url: string; user_id: string };

export default new Flow<Input>({ slug: "analyze_website", maxAttempts: 3 })
  .step({ slug: "website" },
    ({ run })      => scrapeWebsite(run.url))

  .step({ slug: "summary", dependsOn: ["website"] },
    ({ website })  => summarizeWithAI(website.content))

  .step({ slug: "tags",    dependsOn: ["website"] },
    ({ website })  => extractTags(website.content))

  .step({ slug: "saveToDb", dependsOn: ["summary", "tags"] },
    ({ run, summary, tags }) =>
      saveWebsite({ user_id: run.user_id, website_url: run.url, summary, tags }));
```

<details>
  <summary>üí° **Design pattern: How to name your steps effectively**</summary>

  Step naming is an important design decision that affects workflow readability. After analyzing multiple projects, these patterns have proven effective:

  **Recommended approach: Hybrid naming**
  
  - Use **nouns** for steps that produce data other steps depend on
  - Use **verb-noun** combinations for terminal actions or utility steps
  
  ```ts
  // Data-producing steps use nouns
  .step({ slug: "website" }, ...)
  .step({ slug: "summary", dependsOn: ["website"] }, ...)
  
  // Terminal action step uses verb-noun
  .step({ slug: "saveToDb", dependsOn: ["summary"] }, ...)
  ```

  **Why this works well:**
  
  1. When accessing data from dependent steps, nouns create more intuitive property access:
     ```ts
     // Clean and reads naturally
     ({ website }) => summarizeWithAI(website.content) 
     ```
     
  2. Terminal steps that don't produce dependencies benefit from action-oriented naming that clearly describes what they're doing

  **For naming style, use camelCase:**
  ```ts
  .step({ slug: "websiteContent" }, ...)   // Correct 
  .step({ slug: "website_content" }, ...)  // Avoid
  ```

  Step slugs are used as identifiers in TypeScript and must match exactly when referenced in dependency arrays. Following JavaScript conventions with camelCase helps maintain consistency.
  
  While this example uses the hybrid pattern, the most important thing is consistency within your project. Document the chosen convention and apply it throughout the codebase.
</details>

Breaking down this flow definition:

1. First comes the flow's input type definition, requiring a URL and user ID
2. A **root step** scrapes the website, taking input from the flow's parameters
3. Two **parallel steps** (`summary` and `tags`) both depend on the website content
4. A final step waits for both AI operations to complete, then saves everything to the database

### The power of parallel execution

One of pgflow's key advantages is its ability to automatically identify and execute parallel steps. In our workflow:

- Both AI operations depend only on the website content, not on each other
- pgflow recognizes this and runs them simultaneously
- This cuts the total execution time nearly in half compared to running them sequentially
- If one AI call fails (e.g., due to rate limiting), only that step retries - the other continues unaffected
- Each step can have its own retry configuration based on its specific needs

This parallel execution is especially valuable for IO-bound operations like API calls, where concurrent execution dramatically improves throughput.

---

## Step 4 - Compile & migrate

Unlike many workflow systems that interpret definitions at runtime, pgflow compiles your TypeScript flow into native SQL that Postgres can directly execute. This gives you the safety of type checking with the performance of native database execution.

Let's compile our flow and add it to the database:

```bash
# turn the TypeScript DAG into SQL
npx pgflow@0.2.3 compile supabase/functions/_flows/analyze_website.ts

# apply the generated migration to local Postgres
npx supabase@latest migrations up --local
```

This creates a migration file (like `pgflow_2024...sql`) containing all the SQL needed to register your flow in the pgflow system.

:::tip Behind the curtain
Under the hood, the compiler generates a sequence of `pgflow.create_flow` and `pgflow.add_step` function calls that define the workflow structure in your Postgres database. You never need to write this SQL by hand!
:::

---

## Step 5 - Run the stack and test the flow

Now it's time to see our creation in action. We'll need three terminal windows to run the full system:

```bash
# Terminal 1 - start Postgres + Kong + realtime
npx supabase@latest start

# Terminal 2 - serve Edge Functions (this loads the tasks)
npx supabase@latest functions serve

# Terminal 3 - spawn two workers that poll the 'analyze_website' queue
for i in 0 1; do
  curl http://127.0.0.1:54321/functions/v1/analyze_website_worker_$i &
done
```

With everything running, let's trigger our flow from the Supabase SQL editor (or `psql`):

```sql
select * from pgflow.start_flow(
  flow_slug => 'analyze_website',
  input     => '{"url":"https://supabase.com","user_id":"00000000-0000"}'
);
```

### What happens behind the scenes

1. The `pgflow.start_flow` function creates a new run record and queues up your starting step
2. Your Edge Function workers pick up the task and execute the `scrapeWebsite` function
3. When that completes, both AI steps (`summary` and `tags`) are queued and executed in parallel
4. After both AI steps finish, the final `saveToDb` step runs
5. The database has a complete record of each step's execution, timing, and results

### What you should see

In Terminal 2 (functions serve), you'll see the progress logs:

```
[scrapeWebsite] fetching https://supabase.com
[summary] len 15432
[extractTags] run
[saveWebsite] inserting row
```

And in your database, you'll find your new record:

```sql
select website_url, tags, left(summary,60)||'...' as summary
from websites order by id desc limit 1;
```

```
| website_url           | tags                                   | summary                              |
|-----------------------|----------------------------------------|--------------------------------------|
| https://supabase.com  | {postgres,serverless,open-source,...}  | Supabase is an open-source Firebase  |
```

<details>
  <summary>Troubleshooting common issues</summary>

| Symptom | Fix |
|---------|-----|
| `Missing OPENAI_API_KEY` | Export the env var in `supabase/.env` and restart `functions serve`. |
| Row not created | Check `SUPABASE_SERVICE_ROLE_KEY` ‚Äì needs *service-role* level. |
| Workers not running | Ensure you've run the compilation step and applied migrations correctly. |
</details>

---

## What you've accomplished

Congratulations! You've built a sophisticated ETL pipeline with powerful features:

- **Resilient processing** - Each step automatically retries on failure (up to 3 times per our configuration)
- **Parallel execution** - When steps don't depend on each other, they run concurrently
- **Complete audit trail** - Every run's execution history is preserved in your database
- **Modular design** - Each processing step is isolated and independently testable

And you've done all this with under 200 lines of code, without any additional infrastructure beyond your Postgres database!

---

## Where to go from here

This workflow is just the beginning. Here are some ways you could extend it:

- Build a simple React front-end that lets users paste URLs and view the analysis results
- Schedule nightly re-crawls of important websites using a CRON trigger
- Expand the workflow to detect content changes and notify users
- Add additional AI analysis steps like sentiment analysis or entity extraction

We'd love to see what you build with pgflow! ‚≠ê Star [pgflow on GitHub](https://github.com/pgflow-dev/pgflow) and share your creations with our community.
