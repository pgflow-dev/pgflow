---
title: Scraping a website with pgflow and AI
---

## What we will build

A resilient workflow that combines web scraping with OpenAI's powerful text analysis capabilities. You'll create a workflow with 4 tasks (exposed as 4 pgflow steps) that:

1. Fetches a web page content
2. Processes it with **two parallel AI operations** (summary and keyword extraction)
3. Writes everything into your Postgres database

![Analyze Website Workflow](/analyze_website.svg "Analyze website workflow DAG")

The diagram illustrates this workflow. Notice how the AI operations run in parallel after the website content is fetched, significantly reducing the total execution time.

Ever wanted to automatically scrape a website, analyze its content with AI, and store the results in your database? In this tutorial, you'll build exactly that. By the end, you'll see real-time console logs and a neatly stored row in `public.websites`.

:::tip[Why pgflow?]
Unlike traditional solutions that require separate databases and job queues, pgflow stores the entire workflow DAG, retry logic, and execution history directly inside Postgres. While pgflow does require an Edge Worker to process tasks, it's included in the package. This architecture means simpler infrastructure, stronger consistency guarantees, and built-in observability.
:::

## Prerequisites

:::caution[Required setup]
Before you begin, make sure you have:
- A local Supabase project with pgflow installed
- Deno 1.45.2 or newer
- If you haven't set up pgflow yet, follow the [install pgflow guide](/getting-started/install-pgflow/) first
- If you need to create a Supabase project, see the [getting started guide](/getting-started/) first

Note: When using Edge Functions, Supabase automatically provides `SUPABASE_URL` and `SUPABASE_SERVICE_ROLE_KEY` environment variables to your functions.
:::

:::caution[Required OpenAI API Key]
The AI analysis steps in this workflow **require a valid OpenAI API key**. Make sure to:

1. Sign up for an API key at https://platform.openai.com if you don't have one
2. Add your key to `supabase/functions/.env` alongside existing Edge Worker variables:

```diff
# supabase/functions/.env
  EDGE_WORKER_DB_URL=postgres://...
  EDGE_WORKER_LOG_LEVEL=info
+ # OpenAI API key for AI analysis steps
+ OPENAI_API_KEY=sk-...
```

Without this key, the AI analysis steps will fail with authentication errors. For more information about the required environment variables, see the [Getting Started guide](/getting-started/install-pgflow/#set-up-environment-variables).
:::

---

## Step 1 — Create `websites` table

First, create a migration file to set up the database table where the website analysis results will be stored.

### Create a new migration

Create a new migration file in your Supabase project:

```bash
npx supabase migration new add_websites
```

This will create a new file in the `supabase/migrations` directory with a timestamp prefix, like `20240516123456_add_websites.sql`.

### Add the table definition

Open the newly created migration file and add this SQL:

```sql
-- Create the websites table to store analysis results
create table public.websites (
  id           bigserial primary key,
  website_url  text not null,
  summary      text,
  tags         text[],
  created_at   timestamptz default now()
);

```

This table provides everything needed: a unique ID, the website URL, fields for AI-generated content, and a timestamp for when the record was created.

### Apply the migration

Run the migration to create the table in your local database:

```bash
npx supabase migrations up --local
```

You should see confirmation that the migration was applied successfully. You can verify the table was created by checking the Supabase Dashboard's Table Editor or running `select * from public.websites;` in the SQL Editor.

---

## Step 2 — Create task functions

One of pgflow's most powerful features is how it breaks complex workflows into smaller, independently retriable steps. Let's create four specialized task functions:

Create these four task functions in `supabase/functions/_tasks`:
- `scrapeWebsite.ts` - Fetches webpage content
- `summarizeWithAI.ts` - Generates AI summary
- `extractTags.ts` - Extracts keyword tags
- `saveWebsite.ts` - Saves to database

### Web Scraping Function

This function fetches a webpage and strips out HTML tags to create clean text for AI analysis:

<details open>
  <summary><strong>scrapeWebsite.ts</strong> - Fetch and clean webpage content</summary>

```ts
# supabase/functions/_tasks/scrapeWebsite.ts
export default async function scrapeWebsite(url: string) {
  console.log("[scrapeWebsite] fetching", url);

  // Add timeout to prevent requests from hanging (10 seconds)
  const res = await fetch(url, {
    signal: AbortSignal.timeout(10000)
  });

  if (!res.ok) throw new Error(`Fetch failed: ${res.status}`);

  const html = await res.text();
  const text = html.replace(/<[^>]+>/g, " ").replace(/\s+/g, " ").trim();

  // Return just the content, limited to a reasonable size
  return { content: text.slice(0, 25_000) };   // keep token count sane
}
```

:::caution[Production HTML Processing]
The regex-based HTML sanitization is simplified for clarity. For production, use a dedicated library like [html-to-text](https://www.npmjs.com/package/html-to-text) (`npm install html-to-text`) to properly handle complex pages, nested elements, and encoding issues.
:::
</details>

### AI Analysis Functions

Two OpenAI-powered functions analyze the webpage content:

:::tip[OpenAI Responses API with Structured Outputs]
These functions use OpenAI's newer [Responses API](https://platform.openai.com/docs/api-reference/responses) with [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs) to:
- Get type-safe, validated responses directly from the AI model
- Process content in a predictable format through schema validation
- Simplify error handling with automatic validation
:::

<details>
  <summary><strong>summarizeWithAI.ts</strong> - Generate an AI summary</summary>

Generates a concise summary paragraph from the webpage content:

```ts
# supabase/functions/_tasks/summarizeWithAI.ts
import OpenAI from "jsr:@openai/openai@4.33.0";
import { z } from "jsr:@zod/zod";
import { zodTextFormat } from "jsr:@openai/openai@4.33.0/helpers/zod";

const openai = new OpenAI();
const SummarySchema = z.object({ summary: z.string() });

export default async function summarizeWithAI(content: string) {
  console.log("[summarizeWithAI] len", content.length);

  const rsp = await openai.responses.parse({
    model: "gpt-4o",
    input: [
      { role: "system", content: "Return a short paragraph summary." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(SummarySchema, "summary") },
  });

  return rsp.output_parsed.summary;
}
```
</details>

<details>
  <summary><strong>extractTags.ts</strong> - Extract keyword tags</summary>

Extracts 5-10 relevant keyword tags from the content:

```ts
# supabase/functions/_tasks/extractTags.ts
import OpenAI from "jsr:@openai/openai@4.33.0";
import { z } from "jsr:@zod/zod";
import { zodTextFormat } from "jsr:@openai/openai@4.33.0/helpers/zod";

const openai = new OpenAI();
const TagsSchema = z.object({ keywords: z.array(z.string()).max(10) });

export default async function extractTags(content: string) {
  console.log("[extractTags] run");

  const rsp = await openai.responses.parse({
    model: "gpt-4o",
    input: [
      { role: "system",
        content: "Return 5-10 descriptive keywords." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(TagsSchema, "keywords") },
  });

  return rsp.output_parsed.keywords;
}
```
</details>

### Save to Database Function

The final task persists all the analysis results to the database table created in Step 1.

<details>
  <summary><strong>saveWebsite.ts</strong> - Store results in the database</summary>

This function connects to Supabase using environment variables and inserts the website data, AI summary, and tags into the database.

```ts
import { createClient } from "@supabase/supabase-js";

const supabase = createClient(
  Deno.env.get("SUPABASE_URL")!,
  Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!,
);

export default async function saveWebsite(row: {
  website_url: string; summary: string; tags: string[];
}) {
  console.log("[saveWebsite] inserting row");

  const { data } = await supabase
    .from("websites")
    .insert(row)
    .select("*")
    .single()
    .throwOnError();

  console.log("[saveWebsite] inserted with id:", data?.id);

  return data;
}
```

:::note
This function requires the service role key (not the anon key) because it directly inserts data without RLS policies. Both the URL and service role key are automatically available in the Edge Function environment.
:::
</details>

<details>
  <summary>Why separate files instead of one big module?</summary>

  Breaking the workflow into discrete functions provides important benefits:

  1. **Selective retries** - If the OpenAI API rate-limits us, pgflow can retry just the failed AI call without re-scraping the website
  2. **Parallelization** - As you'll see later, the summary and tag extraction can run concurrently
  3. **Clearer error handling** - When something fails, you know exactly which step had the problem
  4. **Better testability** - Each function can be tested in isolation
</details>

---

## Step 3 — Define flow

Now comes the magic - connecting these tasks together using pgflow's TypeScript DSL. This creates a directed acyclic graph (DAG) that explicitly shows the data dependencies between steps.

```ts title="_flows/analyze_website.ts"
import { Flow } from "@pgflow/dsl";
import scrapeWebsite   from "../_tasks/scrapeWebsite.ts";
import summarizeWithAI from "../_tasks/summarizeWithAI.ts";
import extractTags     from "../_tasks/extractTags.ts";
import saveWebsite     from "../_tasks/saveWebsite.ts";

type Input = { url: string };

export default new Flow<Input>({ slug: "analyze_website", maxAttempts: 3 })
  .step({ slug: "website" },
    ({ run })      => scrapeWebsite(run.url))

  .step({ slug: "summary", dependsOn: ["website"] },
    ({ website })  => summarizeWithAI(website.content))

  .step({ slug: "tags",    dependsOn: ["website"] },
    ({ website })  => extractTags(website.content))

  .step({ slug: "saveToDb", dependsOn: ["summary", "tags"] },
    ({ run, summary, tags }) =>
      saveWebsite({ website_url: run.url, summary, tags }));
```

:::note
See [naming steps guide](/how-to/naming-steps/) for best practices on step naming patterns.
:::

Breaking down this flow definition:

1. First comes the flow's input type definition, requiring a URL and user ID
2. A **root step** scrapes the website, taking input from the flow's parameters
3. Two **parallel steps** (`summary` and `tags`) both depend on the website content
4. A final step waits for both AI operations to complete, then saves everything to the database

:::tip[Automatic Parallelization]
pgflow automatically runs the summary and tags steps in parallel since they only depend on the website content and not on each other. This cuts execution time nearly in half and isolates failures - if one AI call fails, only that specific step retries.
:::

:::note
The `maxAttempts: 3` option in the flow definition affects all steps. Each step will automatically retry up to 3 times on failure before being marked as permanently failed.
:::

For a deeper understanding of flow definitions, see the [Flow DSL guide](/concepts/flow-dsl/).

---

## Step 4 — Compile & migrate

Unlike many workflow systems that interpret definitions at runtime, pgflow compiles your TypeScript flow into native SQL that Postgres can directly execute. This gives you the safety of type checking with the performance of native database execution.

Let's compile our flow and add it to the database:

```bash
# turn the TypeScript DAG into SQL
npx pgflow@latest compile supabase/functions/_flows/analyze_website.ts

# apply the generated migration to local Postgres
npx supabase migrations up --local
```

This creates a migration file (like `supabase/migrations/pgflow_2024...sql`) containing all the SQL needed to register your flow in the pgflow system.

:::tip Behind the curtain
Under the hood, the compiler generates a sequence of `pgflow.create_flow` and `pgflow.add_step` function calls that define the workflow structure in your Postgres database. You never need to write this SQL by hand!
:::

---

## Step 5 — Create worker

For your flow to run, you need to create an Edge Worker:

```bash
# Create a worker for your flow
npx supabase functions new analyze_website_worker
```

Replace the contents of the generated index.ts with:

```typescript title="supabase/functions/analyze_website_worker/index.ts"
import { EdgeWorker } from "jsr:@pgflow/edge-worker";
import AnalyzeWebsite from '../_flows/analyze_website.ts';

// Pass the flow definition to the Edge Worker
EdgeWorker.start(AnalyzeWebsite);
```

Disable JWT verification in `supabase/config.toml`:

```diff title="supabase/config.toml"
  [functions.analyze_website_worker]
  enabled = true
- verify_jwt = true
+ verify_jwt = false
```

:::note
This JWT verification is disabled for local development only. In production, you would typically keep JWT verification enabled and start the Edge Worker with a token. For more details on Edge Worker setup and configuration, see the [Run Flow guide](/getting-started/run-flow/).
:::

If needed, add an import map in your `deno.json` file:

```json
// This is NO LONGER NEEDED with JSR imports!
// Shown for reference only if using npm packages
{
  "imports": {
    "npm:openai": "npm:openai@4.33.0"
  }
}
```

:::tip[Using JSR packages]
The examples now use Deno's JSR registry with `jsr:` imports, which means you don't need import maps at all. JSR packages work automatically in Edge Functions without any extra configuration.
:::

---

## Step 6 — Run & test

Now it's time to see our creation in action. We'll need three terminal windows to run the full system:

```bash
# Terminal 1 - start Postgres + Kong + realtime
npx supabase start

# Terminal 2 - serve Edge Functions (this loads the tasks)
npx supabase functions serve

# Terminal 3 - start the Edge Worker (this terminal stays open showing logs)
curl http://127.0.0.1:54321/functions/v1/analyze_website_worker
```

With everything running, let's trigger our flow from the Supabase SQL editor (or `psql`):

```sql
select * from pgflow.start_flow(
  flow_slug => 'analyze_website',
  input     => '{"url":"https://supabase.com"}'
);
```

:::note[Execution Flow]
The Edge Worker executes `scrapeWebsite` first, then runs both AI steps in parallel, and finally saves the results once all analysis is complete. The database maintains a complete execution record.
:::

### What you should see

In Terminal 2 (functions serve), you'll see the progress logs:

```
[scrapeWebsite] fetching https://supabase.com
[summary] len 15432
[extractTags] run
[saveWebsite] inserting row
```

And in your database, you'll find your new record:

```sql
select website_url, tags, left(summary,60)||'...' as summary
from websites order by id desc limit 1;
```

```
| website_url           | tags                                   | summary                              |
|-----------------------|----------------------------------------|--------------------------------------|
| https://supabase.com  | {postgres,serverless,open-source,...}  | Supabase is an open-source Firebase  |
```

---

## What you've built

Congratulations! You've built a sophisticated ETL pipeline with powerful features:

- **Resilient processing** - Each step automatically retries on failure (up to 3 times per our configuration)
- **Parallel execution** - When steps don't depend on each other, they run concurrently
- **Complete audit trail** - Every run's execution history is preserved in your database
- **Modular design** - Each processing step is isolated and independently testable

And you've done all this with under 200 lines of code, without any additional infrastructure beyond your Postgres database!

This workflow is just the beginning. Here are some ways you could extend it:

- Build a simple React front-end that lets users paste URLs and view the analysis results
- Schedule nightly re-crawls of important websites using a CRON trigger
- Expand the workflow to detect content changes and notify users
- Add additional AI analysis steps like sentiment analysis or entity extraction

We'd love to see what you build with pgflow! ⭐ Star [pgflow on GitHub](https://github.com/pgflow-dev/pgflow) and share your creations with our community.

---

## Troubleshooting

<details>
  <summary>Common issues and solutions</summary>

| Symptom | Fix |
|---------|-----|
| `Missing OPENAI_API_KEY` | Export the env var in `supabase/.env` and restart `functions serve`. Check console for error: `Error: Request failed with status code 401 (Unauthorized)` |
| Row not created | Check `SUPABASE_SERVICE_ROLE_KEY` – needs *service-role* level. |
| Workers not running | Ensure you've run the compilation step and applied migrations correctly. |
| `TypeError: fetch failed` | Check your internet connection. Docker containers need network access to reach OpenAI API. |
</details>
