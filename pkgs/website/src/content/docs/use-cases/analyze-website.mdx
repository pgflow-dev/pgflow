---
title: Scraping a website with pgflow and AI
description: Build a 4-step workflow that scrapes a webpage, analyses it with OpenAI, and stores the results
---

import { Steps } from '@astrojs/starlight/components';

**Read:** 15 min | **Implement:** 30 min

## What we will build

You'll build a 4-step workflow that scrapes a page, analyses it with OpenAI, and stores the results.

1. Fetches a web page content
2. Processes it with **two parallel AI operations** (summary and keyword extraction)
3. Writes everything into your Postgres database

See prerequisites below before you start.

<img src="/analyze_website.svg" alt="Analyze Website Workflow" title="Analyze website workflow DAG" style={{ width: '50%' }} />

Diagram of the 4-step DAG

:::tip[Why pgflow?]
Unlike traditional solutions that require separate databases and job queues, pgflow stores the entire workflow DAG, retry logic, and execution history directly inside Postgres. This architecture means simpler infrastructure, stronger consistency guarantees, and built-in observability. See the [Getting Started](/getting-started/) guide for a deeper dive.
:::

## Prerequisites

:::caution[Before you begin]

1. Follow the [install pgflow guide](/getting-started/install-pgflow/) which covers all required steps, versions, and setup instructions. The install command will automatically add the Edge Worker environment variables to your `.env` file.

2. Get an OpenAI API key from the [OpenAI Platform](https://platform.openai.com/api-keys) and add it to your environment:
   ```diff
     # supabase/functions/.env
     EDGE_WORKER_DB_URL=postgres://...
     EDGE_WORKER_LOG_LEVEL=info
   + OPENAI_API_KEY=sk-...
   ```
:::

---

## Step 1 - Create `websites` table

First, create a migration file to set up the database table where the website analysis results will be stored.

<Steps>
1. Create a new migration file and add the table definition:

   ```bash
   npx supabase migration new add_websites
   ```

   Open the newly created migration file and add this SQL:

   ```sql
   -- Create the websites table to store analysis results
   create table public.websites (
     id           bigserial primary key,
     website_url  text not null,
     summary      text,
     tags         text[],
     created_at   timestamptz default now()
   );
   ```

   This table provides everything needed: a unique ID, the website URL, fields for AI-generated content, and a timestamp. See [Supabase Migration Docs](https://supabase.com/docs/reference/cli/supabase-migration-new) for more on migrations.

2. Apply the migration:

   ```bash
   npx supabase migrations up --local

   # Verify the table was created:
   \dt public.websites
   ```
</Steps>

---

## Step 2 - Create task functions

Break complex workflows into smaller, independently retriable steps. Create four specialized task functions:

| File | Responsibility |
| --- | --- |
| `scrapeWebsite.ts` | Fetches webpage content |
| `summarize.ts` | Generates AI summary |
| `extractTags.ts` | Extracts keyword tags |
| `saveWebsite.ts` | Saves to database |

Create these files in `supabase/functions/_tasks`:

### Web Scraping Function

This function fetches a webpage and strips out HTML tags to create clean text for AI analysis:

<details open>
  <summary><strong>scrapeWebsite.ts</strong> - Fetch and clean webpage content</summary>

```ts
// supabase/functions/_tasks/scrapeWebsite.ts
export default async function scrapeWebsite(url: string) {
  console.log("[scrapeWebsite] fetching", url);

  // Add timeout to prevent requests from hanging (10 seconds)
  const res = await fetch(url, {
    signal: AbortSignal.timeout(10000)
  });

  if (!res.ok) throw new Error(`Fetch failed: ${res.status}`);

  const html = await res.text();
  const text = html.replace(/<[^>]+>/g, " ").replace(/\s+/g, " ").trim();

  // Return just the content, limited to a reasonable size
  return { content: text.slice(0, 25_000) };   // keep token count sane
}
```

:::caution[Production HTML Processing]
In production, use [html-to-text](https://www.npmjs.com/package/html-to-text) for full HTML parsing.
:::
</details>

### AI Analysis Functions

Two OpenAI-powered functions analyze the webpage content:

:::tip[OpenAI Responses API with Structured Outputs]
These functions use OpenAI's newer [Responses API](https://platform.openai.com/docs/api-reference/responses) with [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs) to:
- Get type-safe, validated responses directly from the AI model
- Process content in a predictable format through schema validation
:::

<details>
  <summary><strong>summarize.ts</strong> - Generate an AI summary</summary>

Generates a concise summary paragraph from the webpage content:

```ts
// supabase/functions/_tasks/summarize.ts
import OpenAI from "jsr:@openai/openai@4.33.0";
import { z } from "jsr:@zod/zod";
import { zodTextFormat } from "jsr:@openai/openai@4.33.0/helpers/zod";

const openai = new OpenAI();
const SummarySchema = z.object({ summary: z.string() });

export default async function summarize(content: string) {
  console.log("[summarize] processing content");

  const response = await openai.responses.parse({
    model: "gpt-4o",
    input: [
      { role: "system", content: "Return a short paragraph summary." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(SummarySchema, "summary") },
  });

  return response.output_parsed.summary;
}
```
</details>

<details>
  <summary><strong>extractTags.ts</strong> - Extract keyword tags</summary>

Extracts 5-10 relevant keyword tags from the content:

```ts
// supabase/functions/_tasks/extractTags.ts
import OpenAI from "jsr:@openai/openai@4.33.0";
import { z } from "jsr:@zod/zod";
import { zodTextFormat } from "jsr:@openai/openai@4.33.0/helpers/zod";

const openai = new OpenAI();
const TagsSchema = z.object({ keywords: z.array(z.string()).max(10) });

export default async function extractTags(content: string) {
  console.log("[extractTags] extracting keywords");

  const response = await openai.responses.parse({
    model: "gpt-4o",
    input: [
      { role: "system",
        content: "Return 5-10 descriptive keywords." },
      { role: "user", content },
    ],
    text: { format: zodTextFormat(TagsSchema, "keywords") },
  });

  return response.output_parsed.keywords;
}
```
</details>

### Save to Database Function

The final task persists all the analysis results to the database table created in Step 1.

<details>
  <summary><strong>saveWebsite.ts</strong> - Store results in the database</summary>

This function connects to Supabase using environment variables and inserts the website data, AI summary, and tags into the database.

```ts
import { createClient } from "@supabase/supabase-js";

const supabase = createClient(
  Deno.env.get("SUPABASE_URL")!,
  Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!,
);

export default async function saveWebsite(row: {
  website_url: string; summary: string; tags: string[];
}) {
  console.log("[saveWebsite] inserting row");

  const { data } = await supabase
    .from("websites")
    .insert(row)
    .select("*")
    .single()
    .throwOnError();

  console.log("[saveWebsite] inserted with id:", data?.id);

  return data;
}
```

:::note
This function requires the service role key (not the anon key) because it directly inserts data without RLS policies. Both the URL and service role key are automatically available in the Edge Function environment. See [Supabase API Keys](https://supabase.com/docs/guides/api/api-keys) for more details.
:::
</details>

<details>
  <summary>Why separate files instead of one big module?</summary>

  Breaking the workflow into discrete functions provides important benefits:

  1. **Selective retries** - If the OpenAI API rate-limits us, pgflow can retry just the failed AI call without re-scraping the website
  2. **Parallelization** - As you'll see later, the summary and tag extraction can run concurrently
  3. **Clearer error handling** - When something fails, you know exactly which step had the problem
  4. **Better testability** - Each function can be tested in isolation
</details>

---

## Step 3 - Define flow

Connect these tasks using pgflow's TypeScript DSL. Copy to `supabase/functions/_flows/analyze_website.ts`:

```ts title="_flows/analyze_website.ts"
import { Flow } from "@pgflow/dsl";
import scrapeWebsite   from "../_tasks/scrapeWebsite.ts";
import summarize from "../_tasks/summarize.ts";
import extractTags     from "../_tasks/extractTags.ts";
import saveWebsite     from "../_tasks/saveWebsite.ts";

type Input = { url: string };

export default new Flow<Input>({ slug: "analyze_website", maxAttempts: 3 })
  .step({ slug: "website" },
    ({ run })      => scrapeWebsite(run.url))

  .step({ slug: "summary", dependsOn: ["website"] },
    ({ website })  => summarize(website.content))

  .step({ slug: "tags",    dependsOn: ["website"] },
    ({ website })  => extractTags(website.content))

  .step({ slug: "saveToDb", dependsOn: ["summary", "tags"] },
    ({ run, summary, tags }) =>
      saveWebsite({ website_url: run.url, summary, tags }));
```

:::tip[Automatic Parallelization]
pgflow automatically runs the summary and tags steps in parallel since they only depend on the website content and not on each other. This cuts execution time nearly in half and isolates failures - if one AI call fails, only that specific step retries.
:::

The flow dependencies:
- `website` is the root step (no dependencies)
- `summary` depends on `website`
- `tags` depends on `website`
- `saveToDb` depends on both `summary` and `tags`

Breaking down this flow definition:

- Flow input type requires a URL
- **Root step** scrapes the website from flow parameters
- Two **parallel steps** depend on website content
- Final step waits for both AI operations then saves to database

:::note
The `maxAttempts: 3` option in the flow definition affects all steps. Each step will automatically retry up to 3 times on failure before being marked as permanently failed.
:::

For a deeper understanding of flow definitions, see the [Flow DSL guide](/concepts/flow-dsl/).

---

## Step 4 - Compile & migrate

pgflow compiles your TypeScript flow into native SQL that Postgres can directly execute. This gives you the safety of type checking with the performance of native database execution.

Compile your flow and add it to the database:

```bash
# turn the TypeScript DAG into SQL
npx pgflow@latest compile supabase/functions/_flows/analyze_website.ts

# apply the generated migration to local Postgres
npx supabase migrations up --local
```

This creates a migration file containing all the SQL needed to register your flow in the pgflow system. See [Compile flow to SQL](/getting-started/compile-to-sql/) for more details.

:::danger[Adding or removing steps]
When adding or removing steps, you must create a new flow with unique `slug`. See [versioning your flows](/how-to/version-your-flows/) for details.

Currently the process is manual, but the improvements for development environment are coming.
:::

---

## Step 5 - Setup Edge Worker

For your flow to run, you need to create an Edge Worker:

```bash
# Create a worker for your flow
npx supabase functions new analyze_website_worker
```

Replace the contents of the generated index.ts with:

```typescript title="supabase/functions/analyze_website_worker/index.ts"
import { EdgeWorker } from "jsr:@pgflow/edge-worker";
import AnalyzeWebsite from '../_flows/analyze_website.ts';

// Pass the flow definition to the Edge Worker
EdgeWorker.start(AnalyzeWebsite);
```

Disable JWT verification in `supabase/config.toml`:

```diff title="supabase/config.toml"
  [functions.analyze_website_worker]
  enabled = true
- verify_jwt = true
+ verify_jwt = false
```

:::note
This JWT verification is disabled for local development only. In production, you would typically keep JWT verification enabled and start the Edge Worker with a token. For more details on Edge Worker setup and configuration, see the [Run Flow guide](/getting-started/run-flow/).
:::

---

## Step 6 - Run & test

Run three commands to test the flow:

<Steps>
1. Start Supabase and local services:
   ```bash frame="none"
   npx supabase start
   ```

2. Serve the Edge Functions:
   ```bash frame="none"
   npx supabase functions serve
   ```

3. Start the Edge Worker:
   ```bash frame="none"
   curl http://127.0.0.1:54321/functions/v1/analyze_website_worker
   ```
   You should get `ok` in the response; keep this terminal open.
</Steps>

Trigger the flow from the Supabase SQL editor (or `psql`):

```sql frame="terminal" title="Run this in Supabase SQL Editor"
select * from pgflow.start_flow(
  flow_slug => 'analyze_website',
  input     => '{"url":"https://supabase.com"}'
);
```

:::note[Execution Flow]
The Edge Worker executes `scrapeWebsite` first, then runs both AI steps in parallel, and finally saves the results once all analysis is complete. The database maintains a complete execution record.
:::

### What you should see

In Terminal 2 (functions serve), the progress logs show:

```
[scrapeWebsite] fetching https://supabase.com
[summarize] processing content
[extractTags] extracting keywords
[saveWebsite] inserting row
[saveWebsite] inserted with id: 1
```

Check the database for the new record:

```sql
select website_url, tags, left(summary,60)||'...' as summary
from websites order by id desc limit 1;
```

```
| website_url           | tags                                   | summary                              |
|-----------------------|----------------------------------------|--------------------------------------|
| https://supabase.com  | {postgres,serverless,open-source,...}  | Supabase is an open-source Firebase  |
```

---

## What you've built

You've built an ETL pipeline with these features:

- **Resilient processing** - Each step automatically retries on failure (up to 3 times per our configuration)
- **Parallel execution** - When steps don't depend on each other, they run concurrently
- **Complete audit trail** - Every run's execution history is preserved in your database
- **Modular design** - Each processing step is isolated and independently testable


---

## Troubleshooting

<details>
  <summary>Common issues and solutions</summary>

| Symptom | Fix |
|---------|-----|
| `Missing OPENAI_API_KEY` | Export the env var in `supabase/.env` and restart `functions serve`. Check console for error: `Error: Request failed with status code 401 (Unauthorized)` |
| Workers not running | Ensure you've run the compilation step and applied migrations correctly. |
| Row not created | Check `SUPABASE_SERVICE_ROLE_KEY` â€“ needs *service-role* level. |
| `TypeError: fetch failed` | Check your internet connection. Docker containers need network access to reach OpenAI API. |
| `Error: SASL_SIGNATURE_MISMATCH` | Check if DB password is URL-encoded (see [Prepare DB Connection URL](/getting-started/install-pgflow/#prepare-db-connection-url)). |
</details>

For deeper debugging help, see the [Monitor flow execution](/how-to/monitor-flow-execution/) guide.

---

## Discord

Got questions or feedback? We're just getting started, but you can [find us on Discord](https://discord.com/invite/NpffdEyb) if you want to chat about pgflow.
